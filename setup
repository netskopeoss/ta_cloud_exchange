#!/usr/bin/env python3
"""
Release checklist:

1. Set following keys in `LATEST_TAG_AND_VERSION`:
    - CORE_TAG
    - UI_TAG
    - BETA_OPT_IN
    - CORE_LATEST_VERSION_TAG
    - UI_LATEST_VERSION_TAG
    - INSTALL_VERSION
"""
from enum import Enum
import os
import json
import argparse
import ipaddress
from secrets import token_bytes
from base64 import b64encode
import re
import socket
import shutil
import subprocess
import platform
import urllib.request
import urllib.parse
import urllib.error
import uuid
import time
import multiprocessing
import sys
import random
import io
import string

from data.management_server.utils import (
    RECOMMENDED_HOST_OS,
    AVAILABLE_INPUTS,
    SUDO_PREFIX,
    GLUSTERFS_BASE_PORT,
    GLUSTERFS_MAX_PORT,
    CLOUD_EXCHANGE_CONFIG,
    CLOUD_EXCHANGE_CONFIG_KEYS,
    print_fail,
    print_warning,
    print_pass,
    set_proxy,
    read_cloud_exchange_config_file,
    create_secret_file,
    set_directory_permission,
    change_maintenance_password,
    install_gluster,
    ensure_volume_mounted,
    verify_start_create_volume,
    ce_as_vm_check,
    get_secret_location,
    set_sudo_prefix,
    ClientExceptions,
    ServerExceptions,
    configure_logger,
    update_cloudexchange_config,
)

DEFAULT_INPUTS = {
    "CORE_TAG": {
        "default": "crestsystems/netskope:core-latest",
        "skip": True,
        "help": "",
    },
    "UI_TAG": {
        "default": "crestsystems/netskope:ui-latest",
        "skip": True,
        "help": "",
    },
    "MONGO_TAG": {
        "default": "crestsystems/netskope:mongo-latest",
        "skip": True,
        "help": "",
    },
    "RABBITMQ_TAG": {
        "default": "crestsystems/netskope:mongo-latest",
        "skip": True,
        "help": "",
    },
    "UI_PORT": {
        "default": 443,
        "skip": True,
        "help": "",
        "user_input": "Enter the port on which you want to access the Netskope CE UI",
    },
    "CE_MANAGEMENT_PORT": {
        "default": 8000,
        "skip": False,
        "help": "",
        "user_input": "Enter the port on which you want to start CE management server",
    },
    "JWT_SECRET": {
        "default": "",
        "skip": True,
        "help": "",
        "mandatory": True,
        "user_input": "Enter a JWT Secret which will be used for signing the authentication tokens",
    },
    "MAINTENANCE_PASSWORD": {
        "default": "",
        "skip": True,
        # "help": "Enter maintenance password that will be used for docker services. This password can be set only once. ",
        "help": "",
        "mandatory": True,
        "user_input": "Enter maintenance password that will be used for RabbitMQ and MongoDB services (This password can be set only once)",
    },
    "MAINTENANCE_PASSWORD_ESCAPED": {
        "default": "",
        "skip": True,
        "help": "Enter maintenance password that will be used for docker services. This password can be set only once.",
    },
    "WATCHTOWER_TOKEN": {"default": "token", "skip": True, "help": ""},
    "DOCKER_USERNAME": {"default": "", "skip": True, "help": ""},
    "DOCKER_PASSWORD": {"default": "", "skip": True, "help": ""},
    "MAX_MAINTENANCE_WINDOW_MINUTES": {
        "default": 15,
        "skip": True,
        "help": "",
    },
    "PULL_THREADS": {"default": 4, "skip": True, "help": ""},
    "MAX_WAIT_ON_LOCK_IN_MINUTES": {"default": 240, "skip": True, "help": ""},
    "TLS_VERSION": {
        "default": "TLSv1.3",
        "skip": True,
        "help": "",
        "user_input": "Do you want to enable TLSv1.2 along with TLSv1.3 for CE UI",
    },
    "UI_PROTOCOL": {"default": "https", "skip": True, "help": ""},
    "REQUESTS_TIMEOUT": {"default": 300, "skip": True, "help": ""},
    "POPEN_TIMEOUT": {"default": 1800, "skip": True, "help": ""},
    "IS_MPASS_CONFIGURED": {"default": True, "skip": True, "help": ""},
    "MONGO_CURRENT_VERSION": {"default": "", "skip": True, "help": ""},
    "RABBITMQ_COOKIE": {"default": "", "skip": True, "help": ""},
    "LOCATION": {"default": "", "skip": True, "help": ""},
    "COMPOSE_HTTP_TIMEOUT": {"default": 600, "skip": True, "help": ""},
    "CE_AS_VM": {"default": "False", "skip": True, "help": ""},
    "HA_ENABLED": {"default": "False", "skip": True, "help": ""},
    "CONTAINERIZATION_PLATFORM": {"default": "", "skip": True, "help": ""},
    "COMPOSE_VERSION": {"default": "", "skip": True, "help": ""},
    "HOST_OS": {"default": "", "skip": True, "help": ""},
    "PLATFORM_PROVIDER": {"default": "custom", "skip": True, "help": ""},
    "PROMOTION_BANNERS_FILE_LOCATION": {
        "default": "https://raw.githubusercontent.com/netskopeoss/ta_cloud_exchange/main/ce_promotions.json",
        "skip": True,
        "help": "",
    },
    "CE_SETUP_ID": {"default": "", "skip": True, "help": ""},
    "CE_HEX_CODE": {"default": "", "skip": True, "help": ""},
    "CE_IV": {"default": "", "skip": True, "help": ""},
    "CE_SSL_CERTIFICATE_PASSWORD": {"default": "", "skip": True, "help": ""},
    "ENV_FILE": {"default": "./.env", "skip": True, "help": ""},
}
LATEST_TAG_AND_VERSION = {
    "CORE_TAG": "netskopetechnicalalliances/cloudexchange:core6-6.0.0-beta",
    "UI_TAG": "netskopetechnicalalliances/cloudexchange:ui6-6.0.0-beta",
    "MONGO_TAG": "netskopetechnicalalliances/cloudexchange:mongo8-8.0.12",
    "RABBITMQ_TAG": "netskopetechnicalalliances/cloudexchange:rabbitmq4-4.1.3",
    "BETA_OPT_IN": "No",
    "CORE_LATEST_VERSION_TAG": "netskopetechnicalalliances/cloudexchange:core5-latest",
    "UI_LATEST_VERSION_TAG": "netskopetechnicalalliances/cloudexchange:ui5-latest",
    "INSTALL_VERSION": "6.0.0-beta.1",
}
CHECKS = {}
MIN_CPU = 8
MIN_MEM_KB = 0.90 * 16 * 1024**2  # 90% of 16 GB
MEDIUM_PROFILE_REQUIREMENTS = [
    8,
    16,
    80,
    20,
]  # CPU, Memory (in GB), Disk (in GB), Free Disk (in GB)
LARGE_PROFILE_REQUIREMENTS = [
    16,
    32,
    120,
    20,
]  # CPU, Memory (in GB), Disk (in GB), Free Disk (in GB)
CE_PROFILING = {
    MEDIUM_PROFILE_REQUIREMENTS[0]: {
        "name": "Medium",
        "memory": MEDIUM_PROFILE_REQUIREMENTS[1],  # in GB
        "min_memory": MEDIUM_PROFILE_REQUIREMENTS[1]
        * 0.90
        * 1024**2,  # 90% of 16 GB
        "min_disk_bytes": MEDIUM_PROFILE_REQUIREMENTS[2]
        * 0.90
        * 1024**3,  # 90% of 80 GB
    },
    LARGE_PROFILE_REQUIREMENTS[0]: {
        "name": "Large",
        "memory": LARGE_PROFILE_REQUIREMENTS[1],  # in GB
        "min_memory": LARGE_PROFILE_REQUIREMENTS[1]
        * 0.90
        * 1024**2,  # 90% of 32 GB
        "min_disk_bytes": LARGE_PROFILE_REQUIREMENTS[2]
        * 0.90
        * 1024**3,  # 90% of 120 GB
    },
}
MIN_DISK_BYTES = 0.90 * 80 * 1024**3  # 90% of 80 GB
MIN_FREE_DISK_BYTES = 20 * 1024**3
MIN_DOCKER_VERSION = "25.0.3"
MIN_DOCKER_COMPOSE_VERSION = "2.16.0"
MIN_PODMAN_VERSION = "3.4.2"
MIN_PODMAN_COMPOSE_VERSION = "1.0.3"
MIN_OPENSSL_VERSION = "1.1.1"

N = 5
VENV_DIR = ".cevenv"
MIN_PYTHON_MAJOR = 3
MIN_PYTHON_MINOR = 8
MIN_HA_SUPPORTED_VERSION = "5.0.0"
RECOMMENDED_DOCKER_VERSION = "28.3.2"
RECOMMENDED_DOCKER_COMPOSE_VERSION = "2.39.0"
RECOMMENDED_PODMAN_VERSION = "4.9.5"
RECOMMENDED_PODMAN_COMPOSE_VERSION = "1.5.0"

GIT_PLUGIN_REPO = (
    "https://github.com/netskopeoss/ta_cloud_exchange_plugins.git"
)

MONGO_USER_ID = 1001
OLD_MONGO_USER_ID = 999
MONGO_MIGRATION_TIMEOUT = 60
MONGO_LATEST_VERSION = "8.0"
CLOUD_EXCHANGE_LINUX_SERVICE_NAME = "cloud-exchange"

is_ui_running = False
is_management_service_running = False
is_rabbitmq_running = False
is_mongodb_running = False
is_ha = False
CONTAINERIZATION_PLATFORM = None
COMPOSE_VERSION = None
HOST_OS = None
should_ignore = False
avoid_service_restart = False
RABBITMQ_FEATURE_FLAGS = """[classic_mirrored_queue_version,
 classic_queue_type_delivery_support,
 detailed_queues_endpoint,
 direct_exchange_routing_v2,
 drop_unroutable_metric,
 empty_basic_get_metric,
 feature_flags_v2,
 implicit_default_bindings,
 listener_records_in_ets,
 maintenance_mode_status,
 message_containers,
 quorum_queue,
 quorum_queue_non_voters,
 restart_streams,
 stream_filtering,
 stream_queue,
 stream_sac_coordinator_unblock_group,
 stream_single_active_consumer,
 stream_update_config_command,
 tracking_records_in_ets,
 user_limits,
 virtual_host_metadata].
"""


class Status(Enum):
    PASS = ("PASS",)
    NOT_VERIFIED = ("COULDN'T VERIFY",)
    FAIL = "FAIL"


def print_banner():
    print(
        f"""
  _   _        _         _                          ____  _____ 
 | \ | |  ___ | |_  ___ | | __ ___   _ __    ___   / ___|| ____|
 |  \| | / _ \| __|/ __|| |/ // _ \ | '_ \  / _ \ | |    |  _|  
 | |\  ||  __/| |_ \__ \|   <| (_) || |_) ||  __/ | |___ | |___ 
 |_| \_| \___| \__||___/|_|\_\\\___/ | .__/  \___|  \____||_____|
  ___              _          _  _  |_|    _    _               
 |_ _| _ __   ___ | |_  __ _ | || |  __ _ | |_ (_)  ___   _ __  
  | | | '_ \ / __|| __|/ _` || || | / _` || __|| | / _ \ | '_ \ 
  | | | | | |\__ \| |_| (_| || || || (_| || |_ | || (_) || | | |
 |___||_| |_||___/ \__|\__,_||_||_| \__,_| \__||_| \___/ |_| |_|

                                                          v{LATEST_TAG_AND_VERSION["INSTALL_VERSION"]}

    """
    )


def compare_versions(version1, version2):
    versions1 = [int(v) for v in version1.split(".")]
    versions2 = [int(v) for v in version2.split(".")]
    for i in range(max(len(versions1), len(versions2))):
        v1 = versions1[i] if i < len(versions1) else 0
        v2 = versions2[i] if i < len(versions2) else 0
        if v1 > v2:
            return True
        elif v1 < v2:
            return False
    return True

def compare_ce_versions(v1, v2):
    """
    Compare two CE version strings.

    The version string is in the format "A.B.C" or "A.B.C-beta.D.E". The function
    first splits the string into main version and beta version. Then it compares
    the main version first, and the beta version second. If the beta version is
    not present, it is considered to be (0,).

    Returns:
        0 if the versions are equal
        -1 if v1 is less than v2
        1 if v1 is greater than v2
    """
    if not v1:
        return -1
    if not v2:
        return 1

    def version_to_tuple(version):
        parts = version.split("-")
        main_version = parts[0].split(".")
        main_version = tuple(int(part) for part in main_version)
        if len(parts) > 1:
            beta_version = parts[1].split(".")
            beta_version = tuple(
                int(part) for part in beta_version if part.isdigit()
            )
            return main_version + (0,) + beta_version
        else:
            return main_version + (1,)

    v1_tuple = version_to_tuple(v1)
    v2_tuple = version_to_tuple(v2)

    if v1_tuple == v2_tuple:
        return 0
    elif v1_tuple < v2_tuple:
        return -1
    else:
        return 1


def _get_container_name(container_info):
    container_name = container_info.get("Names", "")
    if isinstance(container_name, list):
        container_name = container_name[0]
    return container_name


def strtobool(val):
    val = val.lower()
    if val in ("y", "yes", "t", "true", "on", "1"):
        return 1
    elif val in ("n", "no", "f", "false", "off", "0"):
        return 0
    else:
        raise ValueError("invalid truth value %r" % (val,))


def generate_random_password(length=32):
    chars = string.ascii_letters + string.digits
    return "".join(random.choice(chars) for _ in range(length))


def get_tls_version(version_string):
    versions = version_string.split(',') if version_string else []
    tls_versions = []
    allowed_values = CLOUD_EXCHANGE_CONFIG_KEYS.get("TLS_VERSION", {}).get("allowed_values", [])
    for version in versions:
        version = version.strip()
        if version in allowed_values:
            tls_versions.append(f"TLSv{version}")
        else:
            print_warning(f"Invalid TLS version: {version}. Skipping.")
    if not tls_versions:
        return tls_versions.append(CLOUD_EXCHANGE_CONFIG_KEYS.get("TLS_VERSION", {}).get("default", "TLSv1.3"))

    tls_versions = " ".join(tls_versions)
    tls_versions = f'"{tls_versions}"'
    return tls_versions


def fetch_container_info():
    global is_ui_running, is_rabbitmq_running, is_mongodb_running
    try:
        if isRedHat():
            p = subprocess.Popen(
                ["podman", "ps", "--format", "json"],
                stderr=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stdin=subprocess.PIPE,
            )
            out, err = p.communicate()
            containers = json.loads(out.decode("utf-8"))
            if err:
                raise Exception("Unable to fetch container information.")
        else:
            p = subprocess.Popen(
                ["docker", "ps", "--format", "json"],
                stderr=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stdin=subprocess.PIPE,
            )
            out, err = p.communicate()
            if err:
                raise Exception("Unable to fetch container information.")
            containers = [
                json.loads(line) for line in out.decode("utf-8").splitlines()
            ]

        for container_info in containers:
            if "_mongodb-primary" in _get_container_name(container_info):
                is_mongodb_running = True
            elif "_rabbitmq-stats" in _get_container_name(container_info):
                is_rabbitmq_running = True
            elif "_ui" in _get_container_name(container_info):
                is_ui_running = True
    except Exception:
        print_warning("Unable to fetch container information.")


def check_management_service_running():
    global is_management_service_running
    try:
        p = subprocess.Popen(
            ["systemctl", "is-active", CLOUD_EXCHANGE_LINUX_SERVICE_NAME],
            stderr=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stdin=subprocess.PIPE,
        )
        out, err = p.communicate()
        if err:
            raise Exception("Unable to fetch management service information.")
        status = out.decode("utf-8").splitlines()[0].strip()
        print("Management service status: " + status)
        is_management_service_running = status.lower() == "active"
    except Exception:
        print_warning("Unable to fetch management service information.")


def put_proxy_in_env(location=".env"):
    try:
        with open(location, "w") as f:
            for key, value in AVAILABLE_INPUTS.items():
                f.write(f"{key}={value}\n")
    except Exception as e:
        raise Exception(f"Error occurred while putting proxy variables: {e}")


def upsert_secret_file(update_dictionary, location):
    if not update_dictionary:
        return
    with open(location, "r") as f:
        lines = f.readlines()
        for key, value in update_dictionary.items():
            value = change_maintenance_password(value)
            updated = False
            for i, line in enumerate(lines):
                if line.startswith(f"{key}="):
                    lines[i] = f"{key}={value}\n"
                    updated = True
            if not updated:
                lines.append(f"{key}={value}\n")
    with open(location, "w") as f:
        f.writelines(lines)
    command = f"{SUDO_PREFIX} chmod 444 {location}"
    set_directory_permission(location, command)


def put_env_variable(inputs, location=".env"):
    inputs = dict(inputs)
    # Remove keys that are not in .env
    inputs.pop("CURRENT_DATABASE_VERSION", None)
    if location != ".env":
        inputs.pop("HA_CURRENT_NODE", None)
        inputs.pop("MONGO_CURRENT_VERSION", None)
    try:
        if (
            "MAINTENANCE_PASSWORD" in inputs
            and "MAINTENANCE_PASSWORD_ESCAPED" in inputs
        ):
            inputs["IS_MPASS_CONFIGURED"] = True
        passwords = {}
        is_processed = True if inputs.get("CE_SETUP_ID", None) else False
        secret_location = get_secret_location(inputs)
        if not is_processed:
            passwords["MAINTENANCE_PASSWORD"] = (
                inputs.pop("MAINTENANCE_PASSWORD", None)
                if inputs.get("MAINTENANCE_PASSWORD", None)
                else (
                    get_existing_variable_value(secret_location, "MAINTENANCE_PASSWORD")
                    if os.path.exists(secret_location)
                    else None
                )
            )
            passwords["MAINTENANCE_PASSWORD_ESCAPED"] = (
                inputs.pop("MAINTENANCE_PASSWORD_ESCAPED", None)
                if inputs.get("MAINTENANCE_PASSWORD_ESCAPED", None)
                else (
                    get_existing_variable_value(secret_location, "MAINTENANCE_PASSWORD_ESCAPED")
                    if os.path.exists(secret_location)
                    else None
                )
            )
        if is_processed and inputs.get("MAINTENANCE_PASSWORD", None) and inputs.get("MAINTENANCE_PASSWORD_ESCAPED", None):
            passwords["MAINTENANCE_PASSWORD"] = inputs.pop("MAINTENANCE_PASSWORD", None)
            passwords["MAINTENANCE_PASSWORD_ESCAPED"] = inputs.pop("MAINTENANCE_PASSWORD_ESCAPED", None)
        if (
            passwords.get("MAINTENANCE_PASSWORD", None)
            and passwords.get("MAINTENANCE_PASSWORD_ESCAPED", None)
        ):
            if secret_location:
                create_secret_file(passwords, secret_location)
                inputs["LOCATION"] = secret_location
                AVAILABLE_INPUTS["LOCATION"] = secret_location
                inputs["CE_SETUP_ID"] = AVAILABLE_INPUTS["CE_SETUP_ID"]
                inputs["CE_HEX_CODE"] = AVAILABLE_INPUTS["CE_HEX_CODE"]
                inputs["CE_IV"] = AVAILABLE_INPUTS["CE_IV"]
        if secret_location:
            update_secrets = {}
            ce_ssl_certificate_password = inputs.get("CE_SSL_CERTIFICATE_PASSWORD")
            if ce_ssl_certificate_password:
                update_secrets["CE_SSL_CERTIFICATE_PASSWORD"] = ce_ssl_certificate_password
            upsert_secret_file(update_secrets, secret_location)
        inputs.pop("MAINTENANCE_PASSWORD", None)
        inputs.pop("MAINTENANCE_PASSWORD_ESCAPED", None)
        inputs.pop("MONGO_COMPATIBILITY", None)
        inputs.pop("CE_SSL_CERTIFICATE_PASSWORD", None)
        with open(location, "w") as f:
            for key, value in inputs.items():
                f.write(f"{key}={value}\n")
    except Exception as e:
        raise Exception(f"Error occurred while putting env variables: {e}")


def create_env_if_not_exist(location=".env"):
    try:
        with open(location, "a") as f:
            pass
    except Exception as e:
        raise Exception(f"Error occurred while creating file: {e}")


def execute_shell_command(command, **kwargs):
    """Function to execute shell command using python script."""
    p = None
    try:
        p = subprocess.check_output(command, shell=True, **kwargs)
        return p
    except Exception as e:
        if p:
            p.kill()
        raise Exception(f"Error occurred while executing command. Error: {e}")


def create_mongo_container(
    deployment,
    maintenance_password,
    http_proxy,
    https_proxy,
    temp_container_name,
    image_name,
):
    """Create MongoDB container."""
    create_container = f"{deployment} run -d -t -e MONGODB_ADVERTISED_HOSTNAME=mongodb-primary \
                        -e MONGO_INITDB_ROOT_USERNAME=root \
                        -e MONGO_INITDB_ROOT_PASSWORD={maintenance_password} \
                        -e MONGO_INITDB_DATABASE=cte \
                        -e MONGODB_USERNAME=cteadmin \
                        -e MONGODB_PASSWORD={maintenance_password} \
                        -e HTTP_PROXY=${http_proxy} \
                        -e HTTPS_PROXY=${https_proxy} \
                        -v ./data/mongo-data/data/db:/data/db:z \
                        --name {temp_container_name} \
                        index.docker.io/{image_name} >/dev/null 2>&1"
    execute_shell_command(create_container)


def remove_mongo_container(deployment, temp_mongo_container_name):
    """Remove mongodb container."""
    remove_container = (
        f"{deployment} rm -f {temp_mongo_container_name} >/dev/null 2>&1"
    )
    execute_shell_command(remove_container)
    time.sleep(5)


def execute_mongodb_command(
    deployment,
    temp_mongo_container_name,
    shell,
    maintenance_password,
    eval_command,
    **kwargs,
):
    """Execute command inside mongodb container."""
    compatibility_command = f"{deployment} exec {temp_mongo_container_name} {shell} -u root --password {maintenance_password} admin --eval {eval_command}"
    execute_shell_command(compatibility_command, **kwargs)


def run_temp_mongo_container_for_migration(
    maintenance_password, http_proxy, https_proxy
):
    """Migrate mongo data by running temp mongo container and update featurecompatibilityversion variable from mongo."""
    if platform.system().lower() not in ["darwin"]:
        command = f"{SUDO_PREFIX} chown -R {OLD_MONGO_USER_ID}:{OLD_MONGO_USER_ID} data/mongo-data"
        set_directory_permission("data/mongo-data", command)
    deployment = None
    MONGO_MIGRATE_IMAGE = "mongo:5.0.21"
    MONGO_TEMP_IMAGE_ONE = "mongo:6.0.12"
    MONGO_TEMP_IMAGE_TWO = "mongo:7.0.16"
    MONGO_CURRENT_IMAGE = LATEST_TAG_AND_VERSION["MONGO_TAG"]
    temp_mongo_container_name = "mongo-migration"

    if isRedHat():
        deployment = "podman"
    else:
        deployment = "docker"

    # If from previous temp container is still running then kill that container and start again.
    remove_mongo_container(deployment, temp_mongo_container_name)

    # create mongo container
    create_mongo_container(
        deployment,
        maintenance_password,
        http_proxy,
        https_proxy,
        temp_mongo_container_name,
        MONGO_MIGRATE_IMAGE,
    )
    try:
        time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
        eval_command = (
            """'db.adminCommand({setFeatureCompatibilityVersion: "5.0"})'"""
        )
        execute_mongodb_command(
            deployment,
            temp_mongo_container_name,
            "mongo",
            maintenance_password,
            eval_command,
            stderr=subprocess.PIPE,
        )
    except Exception:
        # remove above container
        pass
    time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
    remove_mongo_container(deployment, temp_mongo_container_name)
    # spin latest mongo version to check compatibility
    create_mongo_container(
        deployment,
        maintenance_password,
        http_proxy,
        https_proxy,
        temp_mongo_container_name,
        MONGO_TEMP_IMAGE_ONE,
    )
    try:
        time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
        eval_command = (
            """'db.adminCommand({setFeatureCompatibilityVersion: "6.0"})'"""
        )
        execute_mongodb_command(
            deployment,
            temp_mongo_container_name,
            "mongosh",
            maintenance_password,
            eval_command,
            stderr=subprocess.PIPE,
        )
    except Exception:
        # remove above container
        pass
    time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
    remove_mongo_container(deployment, temp_mongo_container_name)
    # spin latest mongo version to check compatibility
    create_mongo_container(
        deployment,
        maintenance_password,
        http_proxy,
        https_proxy,
        temp_mongo_container_name,
        MONGO_TEMP_IMAGE_TWO,
    )
    try:
        time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
        eval_command = (
            """'db.adminCommand({setFeatureCompatibilityVersion: "7.0", confirm: true})'"""
        )
        execute_mongodb_command(
            deployment,
            temp_mongo_container_name,
            "mongosh",
            maintenance_password,
            eval_command,
            stderr=subprocess.PIPE,
        )
    except Exception:
        # remove above container
        time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
        remove_mongo_container(deployment, temp_mongo_container_name)
        # update ownership since with CE v5.1.2 mongo custom image has 1001 id
        if platform.system().lower() not in ["darwin"]:
            command = f"{SUDO_PREFIX} chown -R {MONGO_USER_ID}:{MONGO_USER_ID} data/mongo-data"
            set_directory_permission("data/mongo-data", command)
        # spin latest mongo version to check compatibility
        create_mongo_container(
            deployment,
            maintenance_password,
            http_proxy,
            https_proxy,
            temp_mongo_container_name,
            MONGO_CURRENT_IMAGE,
        )

        time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
        eval_command = """'db.adminCommand({getParameter: 1, featureCompatibilityVersion: 1})'"""
        execute_mongodb_command(
            deployment,
            temp_mongo_container_name,
            "mongosh",
            maintenance_password,
            eval_command,
        )

    time.sleep(MONGO_MIGRATION_TIMEOUT // 2)
    remove_mongo_container(deployment, temp_mongo_container_name)
    # update ownership since with CE v5.1.2 mongo custom image has 1001 id
    if platform.system().lower() not in ["darwin"]:
        command = f"{SUDO_PREFIX} chown -R {MONGO_USER_ID}:{MONGO_USER_ID} data/mongo-data"
        set_directory_permission("data/mongo-data", command)
    print_pass("The migration of database has been successfully completed.")


def get_all_existed_env_variable(location=".env", override=True):
    try:
        if not os.path.exists(location):
            return
        with open(location, "r") as f:
            if os.stat(location).st_size > 0:
                with open(f"{location}.{int(time.time())}", "w+") as backup:
                    for line in f.readlines():
                        backup.write(line)
                        key, value = line.split("=", 1)
                        if override or key not in AVAILABLE_INPUTS:
                            AVAILABLE_INPUTS[key] = value.strip()

        if AVAILABLE_INPUTS.get("HTTPS_PROXY"):
            AVAILABLE_INPUTS["CORE_HTTP_PROXY"] = AVAILABLE_INPUTS[
                "HTTPS_PROXY"
            ]
            AVAILABLE_INPUTS["CORE_HTTPS_PROXY"] = AVAILABLE_INPUTS[
                "HTTPS_PROXY"
            ]

            AVAILABLE_INPUTS.pop("HTTP_PROXY", None)
            AVAILABLE_INPUTS.pop("HTTPS_PROXY", None)
        if AVAILABLE_INPUTS.get("RABBITMQ_CUSTOM_CONF_PATH"):
            AVAILABLE_INPUTS.pop("RABBITMQ_CUSTOM_CONF_PATH", None)
    except Exception as e:
        raise Exception(f"Error occurred while getting env variables: {e}")


def _get_cert_location(crt_postfix="cte_cert", is_pem=False, with_ca=False, from_local=False):
    ha_nfs_dir = AVAILABLE_INPUTS.get("HA_NFS_DATA_DIRECTORY")
    base_path = f"{ha_nfs_dir}/config/ssl_certs" if (ha_nfs_dir and not from_local) else "data/ssl_certs"

    cert_file = f"{base_path}/{crt_postfix}.crt"
    key_file = f"{base_path}/{crt_postfix}_key.key"
    pem_file = f"{base_path}/{crt_postfix}_key.pem" if is_pem else None

    if with_ca:
        ca_key = f"{base_path}/{crt_postfix}_ca.key"
        ca_crt = f"{base_path}/{crt_postfix}_ca.crt"
        return cert_file, key_file, pem_file, ca_key, ca_crt

    return cert_file, key_file, pem_file, None, None


def check_for_certs(crt_postfix, is_pem=False, with_ca=False, from_local=False):
    try:
        cert_file, key_file, pem_file, ca_key, ca_crt = _get_cert_location(
            crt_postfix=crt_postfix, is_pem=is_pem, with_ca=with_ca, from_local=from_local
        )
        if (
            os.path.isfile(cert_file)
            and os.path.isfile(key_file)
            and (not is_pem or os.path.isfile(pem_file))
            and (not with_ca or (os.path.isfile(ca_crt) and os.path.isfile(ca_key)))
        ):
            return True
        return False
    except Exception as e:
        raise Exception(
            f"Error occurred while checking for SSL certs. Error: {e}"
        )


def common_ca_self_signed_ssl(ca_key, ca_crt, tls_conf, tls_cert_cert, tls_cert_csr, tls_cert_key, tls_cert_key_pem, common_ca_conf):
    if not os.path.exists(ca_key):
        # Generate self-signed CA certificate
        print(f"ca key not found at {ca_key}, generating new one.")
        command = f"openssl genrsa -out {ca_key} 4096"
        p = subprocess.Popen(
            command.split(),
            stderr=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stdin=subprocess.PIPE,
        )
        _, err = p.communicate()
        if p.returncode != 0:
            raise Exception(f"{err.decode('utf-8')}\n")
        else:
            print(f"{err.decode('utf-8')}\n")
    if not os.path.exists(ca_crt):
        command = f"""openssl req -x509 -new -nodes -key {ca_key} -sha256 -days 3650 -out {ca_crt} \
  -subj "/CN=CloudExchangeCA" \
  -extensions v3_ca -config {common_ca_conf}
  """

        result = subprocess.run(command, shell=True, executable='/bin/bash', 
                              capture_output=True, text=True, check=True)
        err = result.stderr
        if result.returncode != 0:
            raise Exception(f"{err}\n")
        else:
            print(f"{err}\n")

    command = f"openssl req -newkey rsa:4096 -nodes -keyout {tls_cert_key} -out {tls_cert_csr} -config {tls_conf}"
    p = subprocess.Popen(
        command.split(),
        stderr=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stdin=subprocess.PIPE,
    )
    _, err = p.communicate()
    if p.returncode != 0:
        raise Exception(f"{err.decode('utf-8')}\n")
    else:
        print(f"{err.decode('utf-8')}\n")
    command = f"openssl x509 -req -in {tls_cert_csr} -CA {ca_crt} -CAkey {ca_key} -CAcreateserial -out {tls_cert_cert} -days 365 -extfile {tls_conf} -extensions req_ext"

    p = subprocess.Popen(
        command.split(),
        stderr=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stdin=subprocess.PIPE,
    )
    _, err = p.communicate()
    if p.returncode != 0:
        raise Exception(f"{err.decode('utf-8')}\n")
    else:
        print(f"{err.decode('utf-8')}\n")

    command = f"{SUDO_PREFIX} chmod 644 {ca_key} {ca_crt} {tls_cert_cert} {tls_cert_key}"
    set_directory_permission(ca_key, command)

    if tls_cert_key_pem:
        with open(tls_cert_key_pem, "wb") as pem_out:
            with open(tls_cert_cert, "rb") as cert_in:
                shutil.copyfileobj(cert_in, pem_out)
            with open(tls_cert_key, "rb") as key_in:
                shutil.copyfileobj(key_in, pem_out)
        command = f"{SUDO_PREFIX} chmod 644 {tls_cert_key_pem}"
        set_directory_permission(tls_cert_key_pem, command)
    base_path = os.path.dirname(ca_key)
    recursive_chown(base_path, 1001, 1001)
    print(f"Changing owner of {base_path} to user with uid 1001")


def create_self_signed_ssl_certs(cert_file, key_file, extendedkeyusage_conf, combined_pem_file=False, pem_file=None):
    import shutil
    try:
        print("Generating self signed certificate with validity of one year...")
        command = f"openssl req -x509 -newkey rsa:4096 -keyout {key_file} -out {cert_file} -sha256 -days 365 -nodes -subj /CN=localhost -extensions extendedkeyusage -config {extendedkeyusage_conf}"
        p = subprocess.Popen(
            command.split(),
            stderr=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stdin=subprocess.PIPE,
        )
        out, err = p.communicate()
        if p.returncode == 0:
            print(f"{err.decode('utf-8')}\n")
            # Combine cert and key into a single PEM file for MongoDB TLS
            if combined_pem_file and pem_file:
                with open(pem_file, "wb") as pem_out:
                    with open(cert_file, "rb") as cert_in:
                        shutil.copyfileobj(cert_in, pem_out)
                    with open(key_file, "rb") as key_in:
                        shutil.copyfileobj(key_in, pem_out)  
                command = f"{SUDO_PREFIX} chmod 644 {pem_file}"
                set_directory_permission(pem_file, command) 
            command = f"{SUDO_PREFIX} chmod 644 {cert_file}"
            set_directory_permission(cert_file, command)
            command_key = f"{SUDO_PREFIX} chmod 644 {key_file}"
            set_directory_permission(key_file, command_key)
        else:
            raise Exception(f"{err.decode('utf-8')}\n")
    except Exception as e:
        p.kill()
        raise Exception(
            f"Error occurred while generating self-signed ssl certificates. Error: {e}"
        )


def remove_unwanted_fields():
    ha_env_fields = {
        "HA_CURRENT_NODE",
        "DESTINATION_PATH",
        "HA_NFS_DATA_DIRECTORY",
        "LOCATION",
        "CE_SETUP_ID",
        "CE_HEX_CODE",
        "CE_IV",
        "ENV_FILE",
    }
    for key in set(AVAILABLE_INPUTS) - ha_env_fields:
        AVAILABLE_INPUTS.pop(key, None)


def check_port_for_ha(ip, port, timeout=3):
    """
    Check if a specific port on a given IP is reachable.

    Args:
    ip (str): The IP address to check.
    port (int): The port number to check.
    timeout (int): Timeout for the connection attempt in seconds.

    Returns:
    tuple: A tuple containing the following elements:
    - is_port_open (bool): True if the port is open, False otherwise.
    - is_service_running (bool): True if the service is running, False otherwise.
    """
    # Create a socket object
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(timeout)
    is_port_open = False
    is_service_running = False
    try:
        # Try to connect to the given IP and port
        sock.connect((ip, port))
        try:
            # Try to send some data to see if the service responds
            sock.sendall(b"Hello")
            data = sock.recv(1024)
            if data:
                is_port_open = True
                is_service_running = True
            else:
                is_port_open = True
                is_service_running = False
        except socket.error:
            is_port_open = True
            is_service_running = False
    except socket.timeout:
        is_port_open = False
        is_service_running = False
    except socket.error as e:
        if e.errno == 111:  # Connection refused
            is_port_open = True
            is_service_running = False
        else:
            is_port_open = False
            is_service_running = False
    finally:
        sock.close()
    return is_port_open, is_service_running


def check_for_ha_ports(ha_ip_list, ha_current_node):
    try:
        port_flag = True
        try:
            glusterfs_base_port = int(CLOUD_EXCHANGE_CONFIG.get("GLUSTERFS_BASE_PORT", GLUSTERFS_BASE_PORT))
            glusterfs_max_port = int(CLOUD_EXCHANGE_CONFIG.get("GLUSTERFS_MAX_PORT", GLUSTERFS_MAX_PORT))
        except Exception:
            glusterfs_base_port = GLUSTERFS_BASE_PORT
            glusterfs_max_port = GLUSTERFS_MAX_PORT

        for port in [4369, 5671, 15671, 25672, 35672, 24007]:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(("127.0.0.1", port))
            if result == 0:
                if is_rabbitmq_running:
                    print_warning("RabbitMQ container is already running.")
                    break
                else:
                    if port == 24007:
                        print_warning("Port 24007 is already in use, make sure it's being used by GlusterFS.")
                        continue
                    print_fail(f"Port {port} is already in use.")
                    port_flag = False
            else:
                print_pass(f"Port {port} is available.")

        for port in range(glusterfs_base_port, glusterfs_max_port + 1):
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(("127.0.0.1", port))
            if result == 0:
                print_warning(f"Port {port} is already in use, make sure its used by GlusterFS.")
            else:
                print_pass(f"Port {port} is available.")

        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(("127.0.0.1", 27017))
        if result == 0:
            if is_mongodb_running:
                print_warning("MongoDB container is already running.")
            else:
                print_fail(f"Port 27017 is already in use.")
                port_flag = False
        else:
            print_pass(f"Port 27017 is available.")

        CHECKS["Port Check"] = Status.PASS if port_flag else Status.FAIL
    except Exception as e:
        # print(e)
        print_warning(f"Could not verify Port information of the machine")
        CHECKS["Port Check"] = Status.NOT_VERIFIED
    port_error_msg = False
    for host in ha_ip_list.split(","):
        if host != ha_current_node:
            for port in [4369, 5671, 15671, 25672, 35672, 27017, 24007]:
                is_port_open, _ = check_port_for_ha(host, port)
                if not is_port_open:
                    print_fail(
                        f"Port {port} of {host} is not reachable from {ha_current_node}"
                    )
                    port_error_msg = True
                else:
                    print_pass(
                        f"Port {port} of {host} is reachable from {ha_current_node}"
                    )
            for port in range(glusterfs_base_port, glusterfs_max_port + 1):
                is_port_open, _ = check_port_for_ha(host, port)
                if not is_port_open:
                    print_fail(
                        f"Port {port} of {host} is not reachable from {ha_current_node}"
                    )
                    port_error_msg = True
                else:
                    print_pass(
                        f"Port {port} of {host} is reachable from {ha_current_node}"
                    )
    if port_error_msg:
        print(
            "\033[1;31mPlease ensure all HA nodes are reachable from each other.\033[0;37m"
        )
        CHECKS["Port Check"] = Status.FAIL


def print_sizing_table(actual):
    """Print the sizing table."""
    metrics = ["Cores", "RAM (GB)", "Disk Space (GB)", "Free Space (GB)"]
    print(
        "+-----------------+---------------+-----------------+-----------------+"
    )
    print(
        f"| {'Metric':<15} | {'Current':<13} | {'Medium Profile':<15} | {'Large Profile':<15} |"
    )
    print(
        "+-----------------+---------------+-----------------+-----------------+"
    )

    # Print the data rows
    for i in range(len(metrics)):
        print(
            f"| {metrics[i]:<15} | {str(actual[i]):<13} | {str(MEDIUM_PROFILE_REQUIREMENTS[i]):<15} | {str(LARGE_PROFILE_REQUIREMENTS[i]):<15} |"
        )
        print(
            "+-----------------+---------------+-----------------+-----------------+"
        )


def check_machine_specs():
    print("\nVerifying minimum system requirements...")
    print(
        '\033[1;37mNOTE: The actual system requirements depend on several criteria including data volume, # of plugins among others.\nIt is highly recommended to refer to the System Requirements section of the "User Guide".\n'
        + "CE sizing profile is decided based on number of CPU(s) on the machine. 8 CPUs are required for the medium profile and 16 CPUs are required for the large profile.\033[0;37m"
    )

    # Check CPUs
    print("Verifying CPU information...")
    try:
        if not cpu_supports_avx():
            print_fail("CPU does not support AVX")
            CHECKS["CPU AVX Support"] = Status.FAIL
        else:
            print_pass("CPU supports AVX")
            CHECKS["CPU AVX Support"] = Status.PASS
    except Exception:
        print_warning("Could not verify CPU information of the machine...")
        CHECKS["CPU AVX Support"] = Status.NOT_VERIFIED
    try:
        profile_name = None
        cpu_cores = multiprocessing.cpu_count()
        if cpu_cores not in CE_PROFILING:
            CHECKS["CPU Check"] = Status.FAIL
        else:
            profile_name = CE_PROFILING[cpu_cores]["name"]
            CHECKS["CPU Check"] = Status.PASS
    except Exception as e:
        print_warning(f"Could not verify CPU information of the machine...")
        CHECKS["CPU Check"] = Status.NOT_VERIFIED

    # Check Memory
    try:
        total_memory = None
        with open("/proc/meminfo", "r") as f:
            lines = f.readlines()
            for line in lines:
                if line.startswith("MemTotal"):
                    memory = int(line.split()[1])
                    total_memory = round(memory / 1024**2, 1)
                    memory_pass = False
                    if cpu_cores in CE_PROFILING:
                        if memory < CE_PROFILING[cpu_cores]["min_memory"]:
                            CHECKS["Memory Check"] = Status.FAIL
                        else:
                            memory_pass = True
                    else:
                        CHECKS["Memory Check"] = Status.FAIL
                    if memory_pass:
                        CHECKS["Memory Check"] = Status.PASS
                    break
    except Exception as e:
        # print(e)
        print_warning(f"Could not verify Memory information of the machine")
        CHECKS["Memory Check"] = Status.NOT_VERIFIED

    # Check Disk Space
    try:
        # Get the path of the current file
        current_file_path = __file__
        # Get the directory of the current file
        current_dir = os.path.dirname(current_file_path) or os.getcwd()
        disk_stats = shutil.disk_usage(current_dir)
        free_space = disk_stats.free
        total_space = disk_stats.total
        disk_space_check_pass = False
        if cpu_cores in CE_PROFILING:
            min_disk_bytes = CE_PROFILING[cpu_cores]["min_disk_bytes"]
            if total_space < min_disk_bytes:
                CHECKS["Disk Space Check"] = Status.FAIL
            else:
                disk_space_check_pass = True
        else:
            CHECKS["Disk Space Check"] = Status.FAIL

        if disk_space_check_pass:
            CHECKS["Disk Space Check"] = Status.PASS
        try:
            if free_space < MIN_FREE_DISK_BYTES:
                CHECKS["Free Disk Space Check"] = Status.FAIL
            else:
                CHECKS["Free Disk Space Check"] = Status.PASS
        except Exception as e:
            print_warning(f"Could not verify Disk information of the machine")
            CHECKS["Free Disk Space Check"] = Status.NOT_VERIFIED
    except Exception as e:
        # print(e)
        print_warning(f"Could not verify Disk information of the machine")
        CHECKS["Disk Space Check"] = Status.NOT_VERIFIED

    matched = not (
        Status.FAIL
        in [
            CHECKS.get("CPU Check"),
            CHECKS.get("Memory Check"),
            CHECKS.get("Disk Space Check"),
            CHECKS.get("Free Disk Space Check"),
        ]
    )
    sizing_result = None
    if not profile_name and cpu_cores < MEDIUM_PROFILE_REQUIREMENTS[0]:
        profile_name = CE_PROFILING[MEDIUM_PROFILE_REQUIREMENTS[0]]["name"]
    elif not profile_name:
        profile_name = CE_PROFILING[LARGE_PROFILE_REQUIREMENTS[0]]["name"]
    if matched:
        sizing_result = (
            f"\033[0;32mMatched with {profile_name} profile\033[0;37m"
        )
    else:
        sizing_result = (
            f"\033[1;31mFailed for {profile_name} profile\033[0;37m"
        )
    print("\nCE Sizing Profile Check:", sizing_result, end="\n\n")
    if not matched:
        machine_specs = [
            cpu_cores,
            total_memory,
            round(total_space / 1024**3, 1),
            round(free_space / 1024**3, 1),
        ]
        print_sizing_table(machine_specs)

    # Check port availability
    if is_ha or (
        AVAILABLE_INPUTS.get("HA_IP_LIST")
        and AVAILABLE_INPUTS.get("HA_CURRENT_NODE")
    ) or (
        CLOUD_EXCHANGE_CONFIG.get("HA_IP_LIST")
        and CLOUD_EXCHANGE_CONFIG.get("HA_CURRENT_NODE")
    ):
        check_for_ha_ports(
            ha_ip_list=AVAILABLE_INPUTS.get("HA_IP_LIST", "")
            or CLOUD_EXCHANGE_CONFIG.get("HA_IP_LIST", ""),
            ha_current_node=AVAILABLE_INPUTS.get("HA_CURRENT_NODE", "")
            or CLOUD_EXCHANGE_CONFIG.get("HA_CURRENT_NODE", ""),
        )

    if Status.FAIL in [
        CHECKS.get("Memory Check"),
        CHECKS.get("Disk Space Check"),
        CHECKS.get("Free Disk Space Check"),
        CHECKS.get("CPU Check"),
        CHECKS.get("Port Check"),
        CHECKS.get("CPU AVX Support"),
    ]:
        print(
            "\033[1;31mOne or more system requirement checks have failed. Please ensure the minimum system requirements are met to proceed further. \033[0;37m"
        )

    global should_ignore
    if not should_ignore and Status.FAIL in [
        CHECKS.get("Memory Check"),
        CHECKS.get("Disk Space Check"),
        CHECKS.get("Free Disk Space Check"),
        CHECKS.get("CPU Check"),
        CHECKS.get("Port Check"),
        CHECKS.get("CPU AVX Support"),
    ]:
        exit(1)


def check_docker_versions():
    try:
        global CONTAINERIZATION_PLATFORM, COMPOSE_VERSION
        regex = r"(\d+(\.\d+){2,3})"
        command = "docker --version"
        p = subprocess.check_output(command, shell=True)
        docker_version = re.search(regex, p.decode("utf-8")).groups()[0]
        CONTAINERIZATION_PLATFORM = "Docker " + docker_version
        if compare_versions(docker_version, MIN_DOCKER_VERSION):
            print_pass(f"Docker Version {docker_version}")
            CHECKS["Docker"] = Status.PASS
        else:
            print_fail(
                f"Docker Version {docker_version} (Minimum {MIN_DOCKER_VERSION} is required)"
            )
            CHECKS["Docker"] = Status.FAIL
        if docker_version != RECOMMENDED_DOCKER_VERSION:
            print_warning(
                f"The recommended docker version is {RECOMMENDED_DOCKER_VERSION}"
            )

        command = "docker compose version"
        p = subprocess.check_output(command, shell=True)
        docker_compose_version = re.search(regex, p.decode("utf-8")).groups()[
            0
        ]
        COMPOSE_VERSION = "docker compose " + docker_compose_version
        if compare_versions(
            docker_compose_version, MIN_DOCKER_COMPOSE_VERSION
        ):
            print_pass(f"Docker Compose Version {docker_compose_version}")
            CHECKS["Docker-Compose"] = Status.PASS
        else:
            print_fail(
                f"Docker Compose Version {docker_compose_version} (Minimum {MIN_DOCKER_COMPOSE_VERSION} is required)"
            )
            CHECKS["Docker-Compose"] = Status.FAIL
        if docker_compose_version != RECOMMENDED_DOCKER_COMPOSE_VERSION:
            print_warning(
                f"The recommended docker compose version is {RECOMMENDED_DOCKER_COMPOSE_VERSION}"
            )

    except Exception as e:
        CHECKS["Docker-Compose"] = Status.NOT_VERIFIED
        CHECKS["Docker"] = Status.NOT_VERIFIED
        raise Exception("Docker not found")


def check_podman_versions():
    try:
        global CONTAINERIZATION_PLATFORM, COMPOSE_VERSION
        regex = r"(\d+(\.\d+){2,3})"
        command = "podman-compose --version"
        p = subprocess.check_output(command, shell=True)
        lines = p.decode("utf-8").splitlines()

        podman_version = ""
        for line in lines:
            if "podman version" in line:
                podman_version = re.search(regex, line).groups()[0]
                break

        if podman_version == "":
            raise
        CONTAINERIZATION_PLATFORM = "Podman " + podman_version
        if compare_versions(podman_version, MIN_PODMAN_VERSION):
            print_pass(f"Podman Version {podman_version}")
            CHECKS["Podman"] = Status.PASS
        else:
            print_fail(
                f"Podman Version {podman_version} (Minimum {MIN_PODMAN_VERSION} is required)"
            )
            CHECKS["Podman"] = Status.FAIL
        if podman_version != RECOMMENDED_PODMAN_VERSION:
            print_warning(
                f"The recommended podman version is {RECOMMENDED_PODMAN_VERSION}"
            )

        podman_compose_version = ""
        for line in lines:
            if "podman-compose" in line:
                podman_compose_version = re.search(regex, line).groups()[0]
                break

        if podman_compose_version == "":
            raise
        COMPOSE_VERSION = "podman-compose " + podman_compose_version
        if compare_versions(
            podman_compose_version, MIN_PODMAN_COMPOSE_VERSION
        ):
            print_pass(f"Podman Compose Version {podman_compose_version}")
            CHECKS["Podman-Compose"] = Status.PASS
        else:
            print_fail(
                f"Podman Compose Version {podman_compose_version} (Minimum {MIN_PODMAN_COMPOSE_VERSION} is required)"
            )
            CHECKS["Podman-Compose"] = Status.FAIL
        if podman_compose_version != RECOMMENDED_PODMAN_COMPOSE_VERSION:
            print_warning(
                f"The recommended podman-compose version is {RECOMMENDED_PODMAN_COMPOSE_VERSION}"
            )

        command = "rpm -qa"
        p = subprocess.check_output(command, shell=True)
        lines = p.decode("utf-8").splitlines()

        is_plugin_present = False
        for line in lines:
            if line.startswith("podman-plugins"):
                is_plugin_present = True
                break

        if is_plugin_present:
            print_pass(f"Verified podman plugins are installed")
            CHECKS["Podman-Plugin"] = Status.PASS
        else:
            print_fail(
                f"Required podman plugins are not installed. (Run `yum install podman-plugins` and Re-run the script)"
            )
            CHECKS["Podman-Plugin"] = Status.FAIL

    except Exception as e:
        # print(e)
        CHECKS["Podman-Compose"] = Status.NOT_VERIFIED
        CHECKS["Podman-Plugin"] = Status.NOT_VERIFIED
        CHECKS["Podman"] = Status.NOT_VERIFIED
        raise Exception("Podman not found")


def set_ce_version_for_vm():
    try:
        from ce_as_vm_tags import CORE_TAG, UI_TAG, MONGO_TAG, RABBITMQ_TAG

        AVAILABLE_INPUTS["CORE_TAG"] = CORE_TAG
        AVAILABLE_INPUTS["UI_TAG"] = UI_TAG
        AVAILABLE_INPUTS["MONGO_TAG"] = MONGO_TAG
        AVAILABLE_INPUTS["RABBITMQ_TAG"] = RABBITMQ_TAG
        LATEST_TAG_AND_VERSION["CORE_TAG"] = CORE_TAG
        LATEST_TAG_AND_VERSION["UI_TAG"] = UI_TAG
        LATEST_TAG_AND_VERSION["MONGO_TAG"] = MONGO_TAG
        LATEST_TAG_AND_VERSION["RABBITMQ_TAG"] = RABBITMQ_TAG
    except Exception:
        pass


def handle_http_errors(res):
    status_code = res.code
    if status_code == 200:
        # raise ClientExceptions("Request Timeout Error.")
        return

    # Client Errors
    elif status_code == 400:
        raise ClientExceptions("Bad Request Error.")
    elif status_code == 401:
        raise ClientExceptions("Unauthorized Error.")
    elif status_code == 403:
        raise ClientExceptions("Forbidden Error.")
    elif status_code == 404:
        raise ClientExceptions("Not Found Error.")
    elif status_code == 408:
        raise ClientExceptions("Request Timeout Error.")

    # Server Errors
    elif status_code == 500:
        raise ServerExceptions("Internal Server Error.")
    elif status_code == 502:
        raise ServerExceptions("Bad Gateway Error.")
    elif status_code == 503:
        raise ServerExceptions("Service Unavailable Error.")
    elif status_code == 504:
        raise ServerExceptions("Gateway Timeout Error.")


def handle_connectivity(name, url):
    try:
        req = urllib.request.Request(url)
        proxy = None
        if AVAILABLE_INPUTS.get("CORE_HTTPS_PROXY", "") != "":
            proxy = AVAILABLE_INPUTS.get("CORE_HTTPS_PROXY")

        proxies = {}

        if proxy is not None:
            proxies["https"] = proxy

        proxy_support = urllib.request.ProxyHandler(proxies=proxies)
        opener = urllib.request.build_opener(proxy_support)
        urllib.request.install_opener(opener)

        res = urllib.request.urlopen(req, timeout=60)
        handle_http_errors(res)
        CHECKS[name] = Status.PASS
        print_pass(f"Connectivity to {url} verified successfully.")
    except ClientExceptions as e:
        print_fail(
            f"Error occurred while verifying connectivity to {url}: ClientError {e}"
        )
        CHECKS[name] = Status.FAIL
        raise
    except ServerExceptions as e:
        print_fail(
            f"Error occurred while verifying connectivity to {url}: ServerError {e}"
        )
        CHECKS[name] = Status.FAIL
        raise
    except urllib.error.HTTPError as e:
        print_fail(
            f"Error occurred while verifying connectivity to {url}: HTTPError {e}"
        )
        CHECKS[name] = Status.FAIL
        raise
    except urllib.error.URLError as e:
        print_fail(
            f"Error occurred while verifying connectivity to {url}: URLError {e}"
        )
        CHECKS[name] = Status.FAIL
        raise
    except Exception as e:
        print_fail(
            f"Error occurred while verifying connectivity to {url}: {e}"
        )
        CHECKS[name] = Status.FAIL
        raise


def timeout(t, cmd, *args):
    if len(args) != 2:
        return
    pool = multiprocessing.Pool(processes=1)
    result = pool.apply_async(cmd, args=args)
    try:
        retval = result.get(timeout=t)
    except multiprocessing.TimeoutError as err:
        pool.terminate()
        pool.join()
        CHECKS[args[0]] = Status.FAIL
        url = args[1]
        print_fail(
            f"Error occurred while verifying connectivity to {url}: Timed out."
        )
    except Exception as err:
        CHECKS[args[0]] = Status.FAIL
    else:
        return retval


def check_connectivity():
    # Check connectivity for docker
    print(
        "\nVerifying the connectivity for pre-requisite outbound network calls..."
    )

    if not ce_as_vm_check():
        timeout(
            60, handle_connectivity, "Docker Hub", "https://hub.docker.com"
        )

        print(
            "\033[1;37mNOTE: This connectivity check is only for the Docker Hub (https://hub.docker.com). Docker images are pulled from CDN. Please make sure that you have the required connectivity to pull the docker images.\033[0;37m"
        )

    if Status.FAIL in [
        CHECKS.get("Docker Hub"),
    ]:
        print(
            "\033[1;31mOne or more connectivity checks have failed. Verify the network connectivity, proxy settings or firewall configurations to address the connectivity issues. \033[0;37m"
        )


def isRedHat():
    try:
        with open("/etc/redhat-release") as f:
            content = f.readline()
            if content.startswith("Red Hat"):
                return True
            else:
                return False
    except Exception as e:
        return False


def isRedHat79():
    try:
        with open("/etc/redhat-release") as f:
            content = f.readline()
            if content.startswith("Red Hat") and "7.9" in content:
                return True
            else:
                return False
    except Exception as e:
        return False


def cpu_supports_avx():
    with open("/proc/cpuinfo", "r") as f:
        cpuinfo = f.read()
    return "avx" in cpuinfo


def check_prerequisites():
    check_connectivity()
    check_machine_specs()
    print("\nVerifying docker/podman prerequisites...")
    if isRedHat79():
        try:
            check_docker_versions()
        except Exception as e:
            print_fail(
                f"Could not verify Docker/Docker compose version of the machine"
            )
    elif isRedHat():
        try:
            check_podman_versions()
        except Exception as e:
            print_fail(
                f"Could not verify Podman/Podman-compose/Podman-plugins version of the machine"
            )
    else:
        try:
            check_docker_versions()
        except Exception as e:
            try:
                check_podman_versions()
            except Exception as e:
                print_fail(
                    f"Could not verify Docker/Docker compose version of the machine"
                )
                print_fail(
                    f"Could not verify Podman/Podman-compose/Podman-plugins version of the machine"
                )
    print("\nVerifying OpenSSL prerequisites...")
    try:
        process = subprocess.Popen(['openssl', 'version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, error = process.communicate()
        if process.returncode == 0:
            match = re.search(r'^openssl\s*(\d+\.\d+\.\d+)', output.decode("utf-8"), re.IGNORECASE)
            full_version_match = re.search(r'^openssl\s*([\d.]+[a-z]*)\s', output.decode("utf-8"), re.IGNORECASE)
            openssl_version = match.group(1)
            full_version = full_version_match.group(1)
            if compare_versions(openssl_version, MIN_OPENSSL_VERSION):
                print_pass(f"OpenSSL Version {full_version}")
                CHECKS["openssl"] = Status.PASS
            else:
                print_fail(
                    f"OpenSSL Version {full_version} (Minimum {MIN_OPENSSL_VERSION} is required)"
                )
                CHECKS["openssl"] = Status.FAIL
        else:
            print_fail(
                "Could not verify OpenSSL version of the machine."
            )
            CHECKS["openssl"] = Status.FAIL
    except Exception:
        print_fail(
            "Could not verify OpenSSL version of the machine."
        )
        CHECKS["openssl"] = Status.FAIL


def check_and_apply_file_permissions():
    print("\nVerifying required file permissions...")
    # Mongo-Data directory owner
    try:
        if not os.path.exists("data/mongo-data"):
            print_fail(
                f"Could not locate data/mongo-data directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["MONGO"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R {MONGO_USER_ID}:{MONGO_USER_ID} data/mongo-data"
                set_directory_permission("data/mongo-data", command)
                print_pass(
                    f"Changing owner of data/mongo-data to user with uid {MONGO_USER_ID}"
                )
                CHECKS["MONGO"] = Status.PASS

            # mongo_data directory file
            try:
                command = f"{SUDO_PREFIX} chmod -R 744 data/mongo-data"
                set_directory_permission("data/mongo-data", command)
                print_pass(f"Adding read/write permissions to data/mongo-data")
                CHECKS["MONGO-FILE"] = Status.PASS
            except Exception as e:
                print_fail(
                    f"Error occurred while verifying file permissions for data/mongo-data"
                )
                CHECKS["MONGO-FILE"] = Status.FAIL
    except Exception as e:
        # print(e)
        print_fail(f"Error occurred while verifying owner of data/mongo-data")
        CHECKS["MONGO"] = Status.FAIL

    # mongo-init.sh file owner and permissions check
    try:
        if not os.path.exists("data/database-init/mongo-init.sh"):
            print_fail(
                f"Could not locate data/database-init/mongo-init.sh file "
                "(Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["MONGO-INIT"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R {MONGO_USER_ID}:{MONGO_USER_ID} data/database-init"
                set_directory_permission("data/database-init", command)
                print_pass(
                    f"Changing owner of data/database-init/mongo-init.sh to user with uid {MONGO_USER_ID}"
                )
                CHECKS["MONGO-INIT"] = Status.PASS

            # mongo-init.sh permissions check
            try:
                command = f"{SUDO_PREFIX} chmod -R 775 data/database-init"
                set_directory_permission("data/database-init", command)
                print_pass(
                    "Adding read/write permissions to data/database-init/mongo-init.sh"
                )
                CHECKS["MONGO-INIT-FILE"] = Status.PASS
            except Exception as e:
                print_fail(
                    "Error occurred while verifying file permissions for data/database-init/mongo-init.sh"
                )
                CHECKS["MONGO-INIT-FILE"] = Status.FAIL
    except Exception as e:
        print_fail(
            "Error occurred while verifying owner of data/database-init/mongo-init.sh"
        )
        CHECKS["MONGO-INIT"] = Status.FAIL

    # Custom_plugins directory
    try:
        if not os.path.exists("data/custom_plugins"):
            print_fail(
                f"Could not locate data/custom_plugins directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["CUSTOM_PLUGINS"] = Status.FAIL
        elif not os.path.exists("data/custom_plugins/__init__.py"):
            print_fail(
                f"Could not locate data/custom_plugins/__init__.py file (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["CUSTOM_PLUGINS"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R 1001:1001 data/custom_plugins"
                set_directory_permission("data/custom_plugins", command)
                print_pass(
                    f"Changing owner of data/custom_plugins to user with uid 1001"
                )
                CHECKS["CUSTOM_PLUGINS"] = Status.PASS

            try:
                command = f"{SUDO_PREFIX} chmod -R 755 data/custom_plugins"
                set_directory_permission("data/custom_plugins", command)
                print_pass(
                    "Adding read/write permissions to data/custom_plugins"
                )
                CHECKS["CUSTOM_PLUGINS_FILE"] = Status.PASS
            except Exception as e:
                print_fail(
                    f"Error occurred while verifying file permissions for data/custom_plugins"
                )
                CHECKS["CUSTOM_PLUGINS_FILE"] = Status.FAIL

    except Exception as e:
        # print(e)
        print_fail(
            f"Error occurred while verifying file permissions for data/custom_plugins"
        )
        CHECKS["CUSTOM_PLUGINS"] = Status.FAIL

    # ssl_certs directory
    try:
        if not os.path.exists("data/ssl_certs"):
            print_fail(
                f"Could not locate data/ssl_certs directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["SSL_CERTS"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R 1001:1001 data/ssl_certs"
                set_directory_permission("data/ssl_certs", command)
                print_pass(
                    f"Changing owner of data/ssl_certs to user with uid 1001"
                )
                CHECKS["SSL_CERTS"] = Status.PASS

            command = f"{SUDO_PREFIX} chmod 755 data/ssl_certs"
            set_directory_permission("data/ssl_certs", command)
            print_pass(f"Adding read/write permissions to data/ssl_certs")
            CHECKS["SSL_CERTS"] = Status.PASS

        if os.path.exists("data/ssl_certs/cte_cert.crt"):
            command = f"{SUDO_PREFIX} chmod 644 data/ssl_certs/cte_cert.crt"
            set_directory_permission("data/ssl_certs/cte_cert.crt", command)
            print_pass(
                f"Adding read permissions to data/ssl_certs/cte_cert.crt"
            )

        if os.path.exists("data/ssl_certs/cte_cert_key.key"):
            command = f"{SUDO_PREFIX} chmod 644 data/ssl_certs/cte_cert_key.key"
            set_directory_permission(
                "data/ssl_certs/cte_cert_key.key", command
            )
            print_pass(
                f"Adding read permissions to data/ssl_certs/cte_cert_key.key"
            )

    except Exception as e:
        print_fail(
            f"Error occurred while verifying file permissions for data/ssl_certs"
        )
        CHECKS["SSL_CERTS"] = Status.FAIL

    # rabbitmq ssl certs
    try:
        if not os.path.exists("data/ssl_certs/mongodb_rabbitmq_certs"):
            print_fail(
                "Could not locate data/ssl_certs/mongodb_rabbitmq_certs directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["MONGODB_RABBITMQ_SSL_CERTS"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R 1001:1001 data/ssl_certs/mongodb_rabbitmq_certs"
                set_directory_permission("data/ssl_certs/mongodb_rabbitmq_certs", command)
                print_pass("Changing owner of data/ssl_certs/mongodb_rabbitmq_certs to user with uid 1001")
                CHECKS["MONGODB_RABBITMQ_SSL_CERTS"] = Status.PASS

            command = f"{SUDO_PREFIX} chmod 755 data/ssl_certs/mongodb_rabbitmq_certs"
            set_directory_permission("data/ssl_certs/mongodb_rabbitmq_certs", command)
            print_pass("Adding read/write permissions to data/ssl_certs/mongodb_rabbitmq_certs")
            CHECKS["MONGODB_RABBITMQ_SSL_CERTS"] = Status.PASS

        if os.path.exists("data/ssl_certs/mongodb_rabbitmq_certs/tls_cert.crt"):
            command = f"{SUDO_PREFIX} chmod 644 data/ssl_certs/mongodb_rabbitmq_certs/tls_cert.crt"
            set_directory_permission("data/ssl_certs/mongodb_rabbitmq_certs/tls_cert.crt", command)
            print_pass("Adding read permissions to data/ssl_certs/mongodb_rabbitmq_certs/tls_cert.crt")

        if os.path.exists("data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.key"):
            command = f"{SUDO_PREFIX} chmod 644 data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.key"
            set_directory_permission("data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.key", command)
            print_pass("Adding read permissions to data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.key")

        if os.path.exists("data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.pem"):
            command = f"{SUDO_PREFIX} chmod 644 data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.pem"
            set_directory_permission("data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.pem", command)
            print_pass("Adding read permissions to data/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.pem")

    except Exception as e:
        print_fail("Error occurred while verifying file permissions for data/ssl_certs/mongodb_rabbitmq_certs")
        CHECKS["MONGODB_RABBITMQ_SSL_CERTS"] = Status.FAIL

    # rabbitmq directory
    try:
        if not os.path.exists("data/rabbitmq/data"):
            print_fail(
                "Could not locate data/rabbitmq/data directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["RABBITMQ"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R 1001:1001 data/rabbitmq"
                set_directory_permission("data/rabbitmq", command)
                print_pass(
                    "Changing owner of data/rabbitmq to user with uid 1001"
                )
                CHECKS["RABBITMQ"] = Status.PASS
            command = f"{SUDO_PREFIX} chmod -R 744 data/rabbitmq"
            set_directory_permission("data/rabbitmq", command)
            command = f"{SUDO_PREFIX} chmod 755 data/rabbitmq"
            set_directory_permission("data/rabbitmq", command)
            command = f"{SUDO_PREFIX} chmod 755 data/rabbitmq"
            set_directory_permission("data/rabbitmq", command)
            print_pass("Adding read permissions to data/rabbitmq")
            CHECKS["RABBITMQ"] = Status.PASS
    except Exception:
        print_fail(
            "Error occurred while verifying file permissions/owner information for data/rabbitmq directory"
        )
        CHECKS["RABBITMQ"] = Status.FAIL

    try:
        if not os.path.exists("data/ca_certs"):
            print_fail(
                f"Could not locate data/ca_certs directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["ca_certs"] = Status.FAIL
        else:
            command = f"{SUDO_PREFIX} chmod -R 644 data/ca_certs/"
            set_directory_permission("data/ca_certs", command)
            command = f"{SUDO_PREFIX} chmod 755 data/ca_certs"
            set_directory_permission("data/ca_certs", command)
            print_pass(f"Adding read permissions to data/ca_certs")
            CHECKS["ca_certs"] = Status.PASS
    except Exception as e:
        print_fail(
            f"Error occurred while verifying file permissions for data/ca_certs"
        )
        CHECKS["ca_certs"] = Status.FAIL

    try:
        if not os.path.exists("data/files-data"):
            print_fail(
                f"Could not locate data/files-data directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["files_data"] = Status.FAIL
        else:
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R 1001:1001 data/files-data"
                set_directory_permission("data/files-data", command)
                print_pass(
                    "Changing owner of data/files-data to user with uid 1001"
                )
                CHECKS["files_data"] = Status.PASS
            command = f"{SUDO_PREFIX} chmod -R 760 data/files-data/"
            set_directory_permission("data/files-data", command)
            command = f"{SUDO_PREFIX} chmod 760 data/files-data"
            set_directory_permission("data/files-data", command)
            print_pass(f"Adding read and write permissions to data/files-data")
            CHECKS["files_data"] = Status.PASS
    except Exception as e:
        print_fail(
            f"Error occurred while verifying file permissions for data/files-data"
        )
        CHECKS["files_data"] = Status.FAIL

    # start script
    try:
        if not os.path.exists("./start"):
            print_fail(
                f"Could not locate start script (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["START"] = Status.FAIL
        else:
            command = f"{SUDO_PREFIX} chmod 555 ./start"
            set_directory_permission("start", command)
            print_pass(f"Adding execute permissions to start script")
            CHECKS["START"] = Status.PASS
    except Exception as e:
        # print(e)
        print_fail(
            f"Error occurred while verifying file permissions for start script"
        )
        CHECKS["START"] = Status.FAIL

    # stop script
    try:
        if not os.path.exists("./stop"):
            print_fail(
                f"Could not locate stop script (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
            )
            CHECKS["STOP"] = Status.FAIL
        else:
            command = f"{SUDO_PREFIX} chmod 555 ./stop"
            set_directory_permission("stop", command)
            print_pass(f"Adding execute permissions to stop script")
            CHECKS["STOP"] = Status.PASS
    except Exception as e:
        # print(e)
        print_fail(
            f"Error occurred while verifying file permissions for stop script"
        )
        CHECKS["STOP"] = Status.FAIL


def validate_port(port):
    try:
        if port == "":
            return True
        port = int(port)
        if 1 <= port <= 65535:
            return True
        return False
    except Exception as e:
        return False


def check_port(name, port, service_name, is_service_running):
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(("127.0.0.1", port))
        if result == 0:
            if is_service_running:
                if service_name:
                    print_warning(f"The {service_name} is already running.")
                CHECKS[name] = Status.PASS
                return True
            else:
                print_fail(f"Port {port} is already in use.\033[0;37m\n")
                CHECKS[name] = Status.FAIL
                return False
        else:
            print_pass(f"Port {port} is available.\n")
            CHECKS[name] = Status.PASS
            return True
    except Exception as e:
        # print(e)
        print_warning(f"Could not verify Port information of the machine]n")
        CHECKS[name] = Status.NOT_VERIFIED
        return True


def pull_from_github():
    myenv = os.environ.copy()
    myenv["LC_ALL"] = "C"
    myenv["GIT_DIR"] = os.path.join(os.getcwd(), ".git")
    myenv["GIT_WORK_TREE"] = os.getcwd()
    print("\nChecking for local changes..")
    pr1 = subprocess.Popen(
        ["git", "diff", "-G."],
        cwd=os.getcwd(),
        shell=False,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        env=myenv,
    )
    (out, error) = pr1.communicate()
    out = out.decode("utf-8")
    error = error.decode("utf-8")

    print(out)

    if error != "":
        print(f"\nError while checking for git diff. {error}\n")
    if out != "":
        print(
            "\n\033[1;31mThere are some local changes that might affect the installation. Can not proceed further. Please Remove the local changes and re-run the script.\033[0;37m\n"
        )
        exit(1)


def migrate_mongo():
    print("\nVerifying mongo-data directory...")
    if os.path.exists("data/mongo-data/"):
        files = os.listdir("data/mongo-data/")
        files = [x for x in files if not x.startswith(".")]

        if len(files) == 0 or "data" in files:
            print("mongo-data directory structure is already updated... ")
            return

        if not os.path.exists("data/mongo-data/data/db/"):
            print("\nCreating data/mongo-data/data/db directory...")
            os.makedirs("data/mongo-data/data/db/", mode=0o744)

        try:
            for _file in files:
                path_name = f"data/mongo-data/{_file}"
                if os.path.isdir(path_name):
                    shutil.copytree(
                        path_name, f"data/mongo-data/data/db/{_file}"
                    )
                else:
                    shutil.copy(path_name, "data/mongo-data/data/db/")
            print("mongo-data directory structure updated successfully")

            # Change owner to 1001 after copying the data
            if platform.system().lower() not in ["darwin"]:
                command = f"{SUDO_PREFIX} chown -R {MONGO_USER_ID}:{MONGO_USER_ID} data/mongo-data"
                set_directory_permission("data/mongo-data", command)
        except Exception as exc:
            print(f"Error while migrating mongo-data: {exc}")
    else:
        print_fail(
            f"Could not locate data/mongo-data directory (Please run `git reset --hard HEAD && git pull` and re-run the script.)"
        )
        exit(1)


def copy_env(file_path, name):
    destination = "data/ce_env_backups"
    destination = os.path.join(destination, name)
    command = f"cp -r {file_path} {destination}"
    try:
        p = subprocess.Popen(
            command.split(),
            shell=False,
            stderr=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stdin=subprocess.PIPE,
        )
        out, err = p.communicate()
        if len(err) <= 0:
            return
    except Exception as e:
        if p:
            p.kill()
        print(f"Error occurred while taking of backup of {file_path}. Error: {e}")


def take_env_backup(file_path, name):
    copy_env(file_path, name)


def get_total_physical_memory():
    try:
        if sys.platform.startswith("linux"):
            with open("/proc/meminfo", "r") as mem_info:
                for line in mem_info:
                    if line.startswith("MemTotal"):
                        total_memory = int(line.split()[1]) * 1024
                        return total_memory
        elif sys.platform.startswith("darwin"):
            memory_info = os.popen("sysctl hw.memsize").read().strip()
            total_memory = int(memory_info.split(":")[1].strip())
            return total_memory
        return None
    except Exception:
        pass


def assign_memory_to_rabbitmq_container():
    try:
        mem_bytes = get_total_physical_memory()
        if not mem_bytes:
            return
        mem_mib = mem_bytes / (1024**2)

        AVAILABLE_INPUTS["RABBITMQ_RAM_RESOURCES"] = round(mem_mib * (6 / 32))
        AVAILABLE_INPUTS["MONGODB_RAM_RESOURCES"] = round(mem_mib * (4 / 32))
        AVAILABLE_INPUTS["CORE_RAM_RESOURCES"] = round(mem_mib * (18 / 32))
        AVAILABLE_INPUTS["UI_RAM_RESOURCES"] = 256
    except Exception:
        pass


def validate_secret(secret_type, value):
    if not value:
        return False, f"{secret_type} should not be empty."
    restricted_chars = {"\\", "/", " ", "'", '"', "?", "&", "\t", "\n", "#", "$"}
    for char in restricted_chars:
        if char in value:
            return False, (
                f"{secret_type} can not contain \\ / space "
                "tab ' \" ? & # $ characters."
            )
    return True, "Valid"


def set_config_params():
    # Ask for Protocol
    protocol = CLOUD_EXCHANGE_CONFIG.get("UI_PROTOCOL", "https")
    ui_port = CLOUD_EXCHANGE_CONFIG.get("UI_PORT", 443)
    protocol = "https" if protocol not in ["http", "https"] else protocol
    is_ssl_enabled = check_for_certs(crt_postfix="cte_cert")
    if not is_ssl_enabled:
        if protocol == "http":
            DEFAULT_INPUTS["TLS_VERSION"]["default"] = "TLSv1.3"
            DEFAULT_INPUTS["TLS_VERSION"]["skip"] = True
            AVAILABLE_INPUTS["UI_PROTOCOL"] = "http"
        elif protocol == "https":
            AVAILABLE_INPUTS["UI_PROTOCOL"] = "https"
            DEFAULT_INPUTS["UI_PORT"]["default"] = ui_port
            is_ssl_enabled = check_for_certs(crt_postfix="cte_cert")  # Checking if SSL is enabled or not
            if not is_ssl_enabled:
                cert_file, key_file, _, _, _ = _get_cert_location(crt_postfix="cte_cert", is_pem=False, with_ca=False)
                extendedkeyusage_conf = "data/ssl_certs/extendedkeyusage.txt"
                create_self_signed_ssl_certs(
                    cert_file=cert_file, key_file=key_file, extendedkeyusage_conf=extendedkeyusage_conf
                )

            message = f"\033[1;37mReplace {cert_file} and {key_file} files with your custom certificate and key files respectively to use custom SSL certificates.\033[0;37m"
            print(message)
        else:
            print("Invalid input... Valid inputs are 'http' or 'https'.\n")
    else:
        DEFAULT_INPUTS["UI_PORT"]["default"] = ui_port

    if not check_for_certs(crt_postfix="mongodb_rabbitmq_certs/tls_cert", with_ca=True, is_pem=True):
        print("Preparing certificates for secure MongoDB and RabbitMQ...")
        cert_file, key_file, pem_file, ca_key, ca_crt = _get_cert_location(crt_postfix="mongodb_rabbitmq_certs/tls_cert", is_pem=True, with_ca=True)

        common_ca_self_signed_ssl(
            ca_key=ca_key,
            ca_crt=ca_crt,
            tls_conf="data/ssl_certs/mongodb_rabbitmq_certs/extendedkeyusage.txt",
            tls_cert_cert=cert_file,
            tls_cert_csr=cert_file.replace(".crt", ".csr"),
            tls_cert_key=key_file,
            tls_cert_key_pem=pem_file,
            common_ca_conf="data/ssl_certs/mongodb_rabbitmq_certs/common_ca.conf"
        )
        print(f"To change/update mongodb and rabbitmq ssl certs, please remove existing certs from {cert_file}, {key_file}, {pem_file}, {ca_crt} and {ca_key}.\n")

    if not check_for_certs(crt_postfix="mongodb_rabbitmq_certs/tls_cert", with_ca=True, is_pem=True, from_local=True):
        cert_file, key_file, pem_file, ca_key, ca_crt = _get_cert_location(crt_postfix="mongodb_rabbitmq_certs/tls_cert", is_pem=True, with_ca=True)
        local_cert_file, local_key_file, local_pem_file, local_ca_key, local_ca_crt = _get_cert_location(crt_postfix="mongodb_rabbitmq_certs/tls_cert", is_pem=True, with_ca=True, from_local=True)
        base_dir = os.path.dirname(ca_key)
        local_base_dir = os.path.dirname(local_ca_key)
        print(f"Could not found ssl certificates in {local_base_dir}/, Copying SSL certificates from {base_dir}/.")
        execute_shell_command(f"{SUDO_PREFIX} cp -rp {base_dir}/* {local_base_dir}/")
        print(f"To change/update Management server ssl certs, please remove existing certs from {local_cert_file}, {local_key_file}, {local_pem_file}, {local_ca_crt} and {local_ca_key}.")
    else:
        local_cert_file, local_key_file, local_pem_file, local_ca_key, local_ca_crt = _get_cert_location(crt_postfix="mongodb_rabbitmq_certs/tls_cert", is_pem=True, with_ca=True, from_local=True)
        print(f"To change/update Management server ssl certs, please remove existing certs from {local_cert_file}, {local_key_file}, {local_pem_file}, {local_ca_crt} and {local_ca_key}.")

    for key, item in DEFAULT_INPUTS.items():
        data = ""
        if key in CLOUD_EXCHANGE_CONFIG:
            if key == "TLS_VERSION":
                data = get_tls_version(CLOUD_EXCHANGE_CONFIG[key])
                AVAILABLE_INPUTS[key] = data
            elif (
                "IS_MPASS_CONFIGURED" in AVAILABLE_INPUTS
                and key == "MAINTENANCE_PASSWORD"
                and os.path.exists(get_secret_location(AVAILABLE_INPUTS))
            ):
                pass
            elif key == "MAINTENANCE_PASSWORD" and not AVAILABLE_INPUTS.get("MAINTENANCE_PASSWORD", ""):
                if not CLOUD_EXCHANGE_CONFIG[key]:
                    CLOUD_EXCHANGE_CONFIG[key] = generate_random_password()
                    print(f"\nSetting following random Maintenance Password - {CLOUD_EXCHANGE_CONFIG[key]}\n")
                    print("Please note down the above Maintenance Password for future reference. It will be required while migrating Cloud Exchange to different machine.")
                else:
                    status, message = validate_secret("Maintenance Password", CLOUD_EXCHANGE_CONFIG[key])
                    if not status:
                        print(message)
                        CLOUD_EXCHANGE_CONFIG[key] = generate_random_password()
                        print(f"\nSetting following random Maintenance Password - {CLOUD_EXCHANGE_CONFIG[key]}\n")
                        print("Please note down the above Maintenance Password for future reference. It will be required while migrating Cloud Exchange to different machine.")
                AVAILABLE_INPUTS[key] = f"'{CLOUD_EXCHANGE_CONFIG[key]}'"
            elif key == "JWT_SECRET":
                if CLOUD_EXCHANGE_CONFIG[key]:
                    status, message = validate_secret("JWT Secret", CLOUD_EXCHANGE_CONFIG[key])
                    if not status:
                        print(message)
                        CLOUD_EXCHANGE_CONFIG[key] = str(uuid.uuid4().hex).replace("-", "")
                        print(f"\nSetting following random JWT secret - {CLOUD_EXCHANGE_CONFIG[key]}\n")
                        print("Please note down the above JWT secret for future reference. It will be required in Cloud Exchange HA configuration.")
                if not CLOUD_EXCHANGE_CONFIG[key] and not AVAILABLE_INPUTS.get("JWT_SECRET", ""):
                    CLOUD_EXCHANGE_CONFIG[key] = generate_random_password()
                    print(f"\nSetting following random JWT secret - {CLOUD_EXCHANGE_CONFIG[key]}\n")
                    print("Please note down the above JWT secret for future reference. It will be required in Cloud Exchange HA configuration.")
                elif not CLOUD_EXCHANGE_CONFIG[key] and AVAILABLE_INPUTS.get("JWT_SECRET", ""):
                    continue
                AVAILABLE_INPUTS[key] = CLOUD_EXCHANGE_CONFIG[key]
            elif key == "UI_PORT":
                if validate_port(CLOUD_EXCHANGE_CONFIG[key]) and check_port("UI Port Check", CLOUD_EXCHANGE_CONFIG[key],"UI container", is_ui_running):
                    AVAILABLE_INPUTS[key] = CLOUD_EXCHANGE_CONFIG[key]
                elif should_ignore:
                    AVAILABLE_INPUTS[key] = CLOUD_EXCHANGE_CONFIG[key]
            elif key == "CE_MANAGEMENT_PORT":
                if validate_port(CLOUD_EXCHANGE_CONFIG[key]) and check_port("Management Port Check", CLOUD_EXCHANGE_CONFIG[key], "Management service", is_management_service_running):
                    AVAILABLE_INPUTS[key] = CLOUD_EXCHANGE_CONFIG[key]
                elif should_ignore:
                    AVAILABLE_INPUTS[key] = CLOUD_EXCHANGE_CONFIG[key]
            else:
                AVAILABLE_INPUTS[key] = CLOUD_EXCHANGE_CONFIG[key]
            print("")
        data = data.strip() if isinstance(data, str) else data
        if not data:
            if key not in AVAILABLE_INPUTS:
                data = item["default"]
            else:
                data = AVAILABLE_INPUTS[key]
        data = data.strip() if isinstance(data, str) else data
        if key == "MAINTENANCE_PASSWORD_ESCAPED":
            data = urllib.parse.quote_plus(
                AVAILABLE_INPUTS["MAINTENANCE_PASSWORD"].strip("'")
            )

        AVAILABLE_INPUTS[key] = data

        if key == "PULL_THREADS":
            AVAILABLE_INPUTS[key] = item.get("default", 4)

        if (
            key == "RABBITMQ_COOKIE"
            and not AVAILABLE_INPUTS["RABBITMQ_COOKIE"]
        ):
            AVAILABLE_INPUTS["RABBITMQ_COOKIE"] = generate_random_password()


def _validate_ip(ip, debug=True):
    try:
        ipaddress.ip_address(ip)
    except ValueError:
        if debug:
            print(
                "Invalid IP address provided. Please provide a valid IP address."
            )
        return False
    return True


def _validate_ips(ips):
    ips = list(filter(None, [x.strip() for x in ips.split(",")]))
    for ip in ips:
        if not _validate_ip(ip):
            return False
    return True


def _validate_current_node(ip):
    if ip not in AVAILABLE_INPUTS.get("HA_IP_LIST", "").split(","):
        print(
            "Provided HA_CURRENT_NODE doesn't match with any of the listed node addresses."
        )
        return False
    return True


def recursive_change_permissions(root_dir, dir_perms=0o755, file_perms=0o664):
    for dirpath, dirnames, filenames in os.walk(root_dir):
        for dirname in dirnames:
            dir_full_path = os.path.join(dirpath, dirname)
            os.chmod(dir_full_path, dir_perms)
        for filename in filenames:
            file_full_path = os.path.join(dirpath, filename)
            os.chmod(file_full_path, file_perms)


def recursive_chown(path, uid, gid):
    os.chown(path, uid, gid)
    for root, dirs, files in os.walk(path):
        for dir in dirs:
            os.chown(os.path.join(root, dir), uid, gid)
        for file in files:
            os.chown(os.path.join(root, file), uid, gid)


def create_required_files(location):
    if not os.path.exists(f"{location}/config"):
        os.mkdir(f"{location}/config")
    os.chmod(f"{location}/config", 0o744)

    if not os.path.exists(f"{location}/config/ssl_certs"):
        os.mkdir(f"{location}/config/ssl_certs")
    os.chmod(f"{location}/config/ssl_certs", 0o755)
    os.chown(f"{location}/config/ssl_certs", 1001, 1001)

    if os.path.exists(f"{location}/config/ssl_certs/cte_cert.crt"):
        os.chmod(f"{location}/config/ssl_certs/cte_cert.crt", 0o644)

    if os.path.exists(f"{location}/config/ssl_certs/cte_cert_key.key"):
        os.chmod(f"{location}/config/ssl_certs/cte_cert_key.key", 0o644)

    if not os.path.exists(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs"):
        os.mkdir(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs")
    os.chmod(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs", 0o755)
    os.chown(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs", 1001, 1001)

    if os.path.exists(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs/tls_cert.crt"):
        os.chmod(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs/tls_cert.crt", 0o644)

    if os.path.exists(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.key"):
        os.chmod(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.key", 0o644)

    if os.path.exists(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.pem"):
        os.chmod(f"{location}/config/ssl_certs/mongodb_rabbitmq_certs/tls_cert_key.pem", 0o644)

    if not os.path.exists(f"{location}/config/ca_certs"):
        os.mkdir(f"{location}/config/ca_certs")
    os.chmod(f"{location}/config/ca_certs", 0o755)

    if not os.path.exists(f"{location}/config/ca_certs/.keep"):
        open(f"{location}/config/ca_certs/.keep", "w").close()
    os.chmod(f"{location}/config/ca_certs/.keep", 0o644)

    if not os.path.exists(f"{location}/repos"):
        os.mkdir(f"{location}/repos")
    recursive_change_permissions(f"{location}/repos")
    recursive_chown(f"{location}/repos", 1001, 1001)

    if not os.path.exists(f"{location}/plugins"):
        os.mkdir(f"{location}/plugins")
    recursive_change_permissions(f"{location}/plugins")
    recursive_chown(f"{location}/plugins", 1001, 1001)

    if not os.path.exists(f"{location}/custom_plugins"):
        os.mkdir(f"{location}/custom_plugins")
    recursive_change_permissions(f"{location}/custom_plugins")
    recursive_chown(f"{location}/custom_plugins", 1001, 1001)

    if not os.path.isfile(f"{location}/config/mongo_replica.key"):
        with open(f"{location}/config/mongo_replica.key", "w") as fp:
            fp.write(b64encode(token_bytes(60)).decode())
    os.chmod(f"{location}/config/mongo_replica.key", 0o400)
    os.chown(
        f"{location}/config/mongo_replica.key", MONGO_USER_ID, MONGO_USER_ID
    )


def create_persistent_dirs(location="./data"):
    if not os.path.exists(f"{location}/repos"):
        os.mkdir(f"{location}/repos")
    recursive_change_permissions(f"{location}/repos")
    recursive_chown(f"{location}/repos", 1001, 1001)

    if not os.path.exists(f"{location}/plugins"):
        os.mkdir(f"{location}/plugins")
    recursive_change_permissions(f"{location}/plugins")
    recursive_chown(f"{location}/plugins", 1001, 1001)

    if not os.path.exists(f"{location}/custom_plugins"):
        os.mkdir(f"{location}/custom_plugins")
    recursive_change_permissions(f"{location}/custom_plugins")
    recursive_chown(f"{location}/custom_plugins", 1001, 1001)


def update_yaml_file(filename):
    # Import Yaml module for HA.
    import sys
    import os

    # Add virtual environment to path for importing
    venv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), VENV_DIR, "lib", f"python{sys.version_info.major}.{sys.version_info.minor}", "site-packages")
    sys.path.insert(0, venv_path)

    import yaml

    with open(f"{filename}.yml", "r") as file:
        compose_data = yaml.load(file, Loader=yaml.SafeLoader)

    # Add hosts for connectivity
    hosts_list = AVAILABLE_INPUTS["HA_IP_LIST"].split(",")

    # Update hostname of the RabbitMQ container
    for index, var in enumerate(
        compose_data["services"]["rabbitmq-stats"]["environment"]
    ):
        if "RABBITMQ_NODENAME" in var:
            compose_data["services"]["rabbitmq-stats"]["environment"][
                index
            ] = var.replace(
                "rabbit@rabbitmq-stats",
                f"rabbit@{AVAILABLE_INPUTS['HA_CURRENT_NODE']}",
            )
    compose_data["services"]["rabbitmq-stats"]["environment"].extend(
        [
            "RABBITMQ_USE_LONGNAME=true",
            'RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-proto_dist inet_tls -ssl_dist_optfile /etc/rabbitmq/inter_node_tls.conf',
            'RABBITMQ_CTL_ERL_ARGS=-proto_dist inet_tls -ssl_dist_optfile /etc/rabbitmq/inter_node_tls.conf',
        ]
    )
    compose_data["services"]["rabbitmq-stats"]["volumes"].append(
        "./data/rabbitmq/inter_node_tls.conf:/etc/rabbitmq/inter_node_tls.conf:z"
    )

    for service in compose_data["services"]:
        compose_data["services"][service]["env_file"] = [
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/.env",
            "${LOCATION}",
        ]

    # Add Ports
    compose_data["services"]["rabbitmq-stats"]["ports"] = [
        "4369:4369",
        "5671:5671",
        "15671:15671",
        "25672:25672",
        "35672:35672",
    ]
    compose_data["services"]["mongodb-primary"]["ports"] = ["27017:27017"]
    # Update volume path for RabbitMQ HA
    for index, volume in enumerate(compose_data["services"]["rabbitmq-stats"]["volumes"]):
        compose_data["services"]["rabbitmq-stats"]["volumes"][index] = volume.replace(
            "./data/ssl_certs",
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/ssl_certs",
        )
    # Update volume path for MongoDB HA
    for index, volume in enumerate(compose_data["services"]["mongodb-primary"]["volumes"]):
        compose_data["services"]["mongodb-primary"]["volumes"][index] = volume.replace(
            "./data/ssl_certs",
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/ssl_certs",
        )

    # Update mongo service for authentication using mongo_replica.key
    compose_data["services"]["mongodb-primary"]["volumes"].append(
        f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/mongo_replica.key:/data/mongo_replica.key:z"
    )
    compose_data["services"]["mongodb-primary"][
        "entrypoint"
    ][2] += " --bind_ip_all --replSet mongo_replica_set --keyFile /data/mongo_replica.key --tlsAllowInvalidHostnames"

    # Update connection strings and add required variables
    compose_data["services"]["core"]["environment"].extend(
        [
            "HA_IP_LIST=${HA_IP_LIST}",
            "UI_PROTOCOL=${UI_PROTOCOL}",
            "UI_PORT=${UI_PORT}",
        ]
    )

    # Update the certificate paths
    for index, volume in enumerate(compose_data["services"]["ui"]["volumes"]):
        compose_data["services"]["ui"]["volumes"][index] = volume.replace(
            "./data/ssl_certs",
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/ssl_certs",
        )
    for index, volume in enumerate(
        compose_data["services"]["core"]["volumes"]
    ):
        volume = volume.replace(
            "./data/ca_certs",
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/ca_certs",
        )
        volume = volume.replace(
            "./data/custom_plugins",
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/custom_plugins",
        )
        volume = volume.replace(
            "./data/ssl_certs",
            f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/config/ssl_certs",
        )
        compose_data["services"]["core"]["volumes"][index] = volume

    compose_data["volumes"] = {
        "nfs_repos": {
            "driver": "local",
            "driver_opts": {
                "type": "none",
                "o": "bind",
                "device": f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/repos",
            },
        },
        "nfs_plugins": {
            "driver": "local",
            "driver_opts": {
                "type": "none",
                "o": "bind",
                "device": f"{AVAILABLE_INPUTS['HA_NFS_DATA_DIRECTORY']}/plugins",
            },
        },
    }

    with open(f"{filename}-ha.yml", "w") as file:
        yaml.dump(
            compose_data,
            file,
            default_flow_style=False,
            sort_keys=False,
            Dumper=yaml.SafeDumper,
        )


def ask_for_ha_values(primary=False):
    global CLOUD_EXCHANGE_CONFIG
    # Ask for IP list
    key = "HA_IP_LIST"
    data = CLOUD_EXCHANGE_CONFIG.get(key) or AVAILABLE_INPUTS.get(key, "")
    data = ",".join(
        list(filter(None, [i.strip() for i in data.split(",")]))
    )
    if not data:
        print(
            f"{key} is not provided. Please provide a valid value."
        )
    AVAILABLE_INPUTS[key] = data

    # Ask for current node info
    key = "HA_CURRENT_NODE"
    data = CLOUD_EXCHANGE_CONFIG.get(key) or AVAILABLE_INPUTS.get(key, "")
    if not data:
        print(
            "HA_CURRENT_NODE is not provided. Please provide a valid value."
        )
    if _validate_current_node(data):
        AVAILABLE_INPUTS[key] = data
        if primary:
            AVAILABLE_INPUTS["HA_PRIMARY_NODE_IP"] = data
    else:
        print(
            "The provided values does not exist in above list. Please provide a valid value."
        )


def generate_dependency_message(version_dict):
    required_versions = []
    for dep, versions in version_dict.items():
        version_string = []
        if "min_version" in versions:
            version_string.append(f">={versions['min_version']}")
        if "max_version" in versions:
            version_string.append(f"<={versions['max_version']}")
        required_versions.append(dep + ", ".join(version_string))
    return " | ".join(required_versions)


def check_python_prerequisites():
    print("\nVerifying the Python prerequisites...")
    python_script_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), VENV_DIR, "bin", "python")

    version_dict = {
        "pyyaml": {"min_version": "6.0.0"},
        "python-dotenv": {"min_version": "0.20.0", "max_version": "1.0.0"},
        "pymongo": {"min_version": "4.6.3", "max_version": "4.7.3"},
    }

    def get_version_spec(constraints):
        spec = ""
        if "min_version" in constraints:
            spec += f">={constraints['min_version']}"
        if "max_version" in constraints:
            spec += f",<={constraints['max_version']}" if spec else f"<={constraints['max_version']}"
        return spec

    def is_version_valid(package, constraints):
        try:
            # Get package version using the venv's Python
            result = subprocess.run(
                [".cevenv/bin/python", "-c", f"import importlib.metadata; print(importlib.metadata.version('{package}'))"],
                capture_output=True,
                text=True,
                check=True
            )
            version = result.stdout.strip()
            if "min_version" in constraints and not compare_versions(version, constraints["min_version"]):
                return False, f"current {version} < min {constraints['min_version']}"
            if "max_version" in constraints and not compare_versions(constraints["max_version"], version):
                return False, f"current {version} > max {constraints['max_version']}"
            return True, version
        except subprocess.CalledProcessError:
            return False, "package not installed"
        except Exception as e:
            return False, f"check failed: {e}"

    try:
        # Check and install packages
        for package, constraints in version_dict.items():
            valid, info = is_version_valid(package, constraints)
            if not valid:
                if ce_as_vm_check():
                    raise Exception("Incompatible versions found for " + package)
                print(f"Installing/Updating {package}: reason: {info}")
                version_spec = get_version_spec(constraints)
                install_cmd = [python_script_path, "-m", "pip", "install", "--upgrade", f"{package}{version_spec}"]
                try:
                    subprocess.run(install_cmd, check=True, capture_output=True, text=True)
                    print_pass(f"Successfully installed/updated {package}")
                except subprocess.CalledProcessError as e:
                    print_fail(f"Failed to install/update {package}: {e.stderr}")
                    exit(1)

        # Final verification
        for package, constraints in version_dict.items():
            valid, info = is_version_valid(package, constraints)
            if not valid:
                raise Exception(f"Dependency verification failed for {package}: {info}")

        print_pass("Python prerequisites verified successfully.")

    except Exception as err:
        if ce_as_vm_check():
            print_fail(f"{err}. Please make sure the dependencies ({generate_dependency_message(version_dict)}) are satisfied before running ./setup script.")
        else:
            print_fail(f"Error encountered while installing ({generate_dependency_message(version_dict)}). Error: {err}")
        exit(1)


def set_up_ha_values():
    global is_ha, CLOUD_EXCHANGE_CONFIG
    # Base version for HA support is 5.0.0
    install_version = AVAILABLE_INPUTS["INSTALL_VERSION"]
    if install_version < MIN_HA_SUPPORTED_VERSION:
        return

    try:
        is_ha = CLOUD_EXCHANGE_CONFIG.get("HA_ENABLED", False) or (True if AVAILABLE_INPUTS.get("HA_ENABLED", "False").strip().lower() == "true" else False)
    except Exception:
        print("Invalid input. Please provide a valid input.")
    if not is_ha:
        AVAILABLE_INPUTS.pop("HA_NFS_DATA_DIRECTORY", None)
        AVAILABLE_INPUTS.pop("HA_IP_LIST", None)
        AVAILABLE_INPUTS.pop("HA_CURRENT_NODE", None)
        AVAILABLE_INPUTS.pop("HA_PRIMARY_NODE_IP", None)
        # previously we have HA and now we want to setup on-prem/OVA.
        if (
            "IS_MPASS_CONFIGURED" in AVAILABLE_INPUTS.keys()
            and AVAILABLE_INPUTS.get("LOCATION", None)
            and AVAILABLE_INPUTS.get("LOCATION", None)
            not in [".env.keys", "/etc/.env.keys"]
        ):
            dest_location = None
            env_file_location = None
            env_file = None
            if ce_as_vm_check():
                dest_location = "/etc/"
                env_file_location = "/etc/.env.keys"
                env_file = "./.env"
            else:
                dest_location = "."
                env_file_location = ".env.keys"
                env_file = "./.env"
            if os.path.isfile(AVAILABLE_INPUTS.get('LOCATION')):
                command = f"{SUDO_PREFIX} mv {AVAILABLE_INPUTS.get('LOCATION', None)} {dest_location}"
                execute_shell_command(command)
            AVAILABLE_INPUTS["LOCATION"] = env_file_location
            AVAILABLE_INPUTS["ENV_FILE"] = env_file
        return

    check_python_prerequisites()

    location = CLOUD_EXCHANGE_CONFIG.get("HA_NFS_DATA_DIRECTORY", "").strip().rstrip("/") or AVAILABLE_INPUTS.get(
        "HA_NFS_DATA_DIRECTORY", ""
    )
    if os.path.isdir(location):
        create_required_files(location)
        # previously we have on-prem / OVA and now we want to setup HA.
        if is_ha and AVAILABLE_INPUTS.get("LOCATION", None) in [
            ".env.keys",
            "/etc/.env.keys",
        ]:
            if os.path.isfile(AVAILABLE_INPUTS.get('LOCATION')):
                command = f"{SUDO_PREFIX} mv {AVAILABLE_INPUTS.get('LOCATION')} {location}/config/"
                execute_shell_command(command)
            AVAILABLE_INPUTS["LOCATION"] = f"{location}/config/.env.keys"
            AVAILABLE_INPUTS["ENV_FILE"] = f"{location}/config/.env"

        # previously we have ha with .env.keys stored at $HA_NFS_DATA_DIRECTORY/.env.keys now its stored at
        # $HA_NFS_DATA_DIRECTORY/config/.env.keys
        if (
            "IS_MPASS_CONFIGURED" in AVAILABLE_INPUTS.keys()
            and AVAILABLE_INPUTS.get("LOCATION", None)
            and AVAILABLE_INPUTS.get("LOCATION", None)
            not in [".env.keys", "/etc/.env.keys"]
            and AVAILABLE_INPUTS.get("LOCATION", None) != f"{location}/config/.env.keys"
        ):
            if os.path.isfile(AVAILABLE_INPUTS.get('LOCATION')):
                command = f"{SUDO_PREFIX} mv {AVAILABLE_INPUTS.get('LOCATION')} {location}/config/"
                execute_shell_command(command)
            AVAILABLE_INPUTS["LOCATION"] = f"{location}/config/.env.keys"
            AVAILABLE_INPUTS["ENV_FILE"] = f"{location}/config/.env"

    else:
        print("The provided path doesn't exist. Please provide a valid path.")
        exit(1)
    try:
        get_all_existed_env_variable(
            f"{location}/config/.env", override=False
        )  # Get env variables from shared location
    except Exception:
        pass  # For fresh setup, shared .env doesn't exist
    AVAILABLE_INPUTS["HA_NFS_DATA_DIRECTORY"] = location

    ask_for_ha_values(primary=True)

    update_yaml_file("docker-compose")
    if not ce_as_vm_check():
        update_yaml_file("podman-compose")


def parse_selinux_status(output):
    selinux_info = {}
    lines = output.splitlines()
    for line in lines:
        key, value = map(str.strip, line.split(":"))
        selinux_info[key] = value
    return selinux_info


def mongo_feature_compatibility(maintenance_password):
    """
    Make mongo data compatibility when migrating from older to newer version.
    """
    current_version = get_existing_variable_value(".env", "MONGO_CURRENT_VERSION")
    is_mongo_compatible = (
        True
        if current_version and compare_versions(current_version, MONGO_LATEST_VERSION)
        else False
    )
    mongo_data_path = "data/mongo-data/data/db/"
    if not is_mongo_compatible and len(os.listdir(mongo_data_path)) > 1:
        print()
        print_warning(
            "The migration process for database is currently underway. Kindly wait for a few minutes."
        )
        run_temp_mongo_container_for_migration(
            maintenance_password,
            AVAILABLE_INPUTS.get("CORE_HTTP_PROXY", ""),
            AVAILABLE_INPUTS.get("CORE_HTTPS_PROXY", ""),
        )
    AVAILABLE_INPUTS["MONGO_CURRENT_VERSION"] = MONGO_LATEST_VERSION


def is_selinux_enabled():
    try:
        process = subprocess.Popen(
            ["sestatus"], stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        stdout, _ = process.communicate()
        if process.returncode == 0:
            status_output = stdout.decode("utf-8").strip()
            selinux_info = parse_selinux_status(status_output)
            return (
                selinux_info.get("SELinux status", "").strip().lower()
                == "enabled"
            )
        else:
            return False
    except Exception:
        return False


def get_existing_variable_value(filepath, key_name):
    """Retrieve the value of an environment variable from a specific file path."""
    try:
        with open(filepath, "r") as f:
            if os.stat(filepath).st_size > 0:
                for line in f.readlines():
                    key, value = line.split("=", 1)
                    if key == key_name:
                        return value.strip()
    except Exception as e:
        raise Exception(
            f"Error occured while getting maintenance password. Error: {e}."
        )


def get_os_name_and_major_version():
    try:
        pretty_name = ""
        version_id = ""
        os_name = ""
        os_version = ""
        allow = False
        if os.path.exists("/etc/os-release"):
            with open("/etc/os-release", "r") as f:
                for line in f:
                    if line.startswith("PRETTY_NAME="):
                        pretty_name = line.split("=")[1].strip().strip('"')
                    elif line.startswith("VERSION_ID="):
                        version_id = line.split("=")[1].strip().strip('"')
                    if pretty_name and version_id:
                        break
            if "ubuntu" in pretty_name.lower():
                if version_id.split(".")[0] in ["20", "22", "24"]:
                    os_version = version_id.split(".")[0]
                os_name = "Ubuntu"
                allow = True
            elif "centos" in pretty_name.lower():
                if version_id.split(".")[0] in ["7", "8"]:
                    os_version = version_id.split(".")[0]
                os_name = "CentOS"
            elif "red hat" in pretty_name.lower():
                if version_id.split(".")[0] in ["7", "8", "9"]:
                    os_version = version_id.split(".")[0]
                os_name = "RHEL"
                allow = True
        os_name_and_major_version = f"{os_name} {os_version}".strip()
        if os_name_and_major_version not in RECOMMENDED_HOST_OS and allow:
            print_warning(
                f"WARNING: CE is not supported on {pretty_name} version. Please switch to one of the supported version of OS. Supported OS: {RECOMMENDED_HOST_OS}"
            )
        elif os_name_and_major_version not in RECOMMENDED_HOST_OS:
            print(
                f"\033[1;31mCE is not supported on {pretty_name}. Please switch to one of the supported OS. Supported OS: {RECOMMENDED_HOST_OS} \033[0;37m"
            )
            if not should_ignore:
                exit(1)
        return os_name_and_major_version
    except Exception:
        pass
    return ""


def collect_host_platform_details():
    gcp = "gcp"
    aws = "aws"
    azure = "azure"
    vmware = "vmware"
    microsoft = "microsoft" # Hyper-V
    custom = "custom"

    base_url = "http://169.254.169.254"

    def check_platform(url, headers, machine_type):
        try:
            url_obj = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(url_obj, timeout=5) as response:
                if response.getcode() == 200:
                    return machine_type
                return False
        except Exception:
            return False

    def check_gcp():
        headers = {"Metadata-Flavor": "Google"}
        url = f"{base_url}/computeMetadata/v1/instance/hostname"

        return check_platform(url, headers, gcp)

    # check functon
    def check_ce_vm_marker_file():
        file_path = "/.cloud_exchange_vm.marker"
        if ce_as_vm_check():
            with open(file_path, "r") as file:
                lines = file.readlines()
                if lines:
                    return lines[0].strip().lower()

    def check_aws():
        # request for token
        headers = {"X-aws-ec2-metadata-token-ttl-seconds": "21600"}
        token_url = f"{base_url}/latest/api/token"
        TOKEN = None
        try:
            request = urllib.request.Request(
                token_url, headers=headers, method="PUT"
            )
            with urllib.request.urlopen(request, timeout=5) as response:
                TOKEN = response.read().decode("utf-8")

        except Exception:
            return False

        # request for metadata
        headers = {"X-aws-ec2-metadata-token": f"{TOKEN}"}
        metadata_url = f"{base_url}/latest/meta-data/local-hostname"
        return check_platform(metadata_url, headers, aws)

    def check_azure():
        headers = {"Metadata": "true"}
        url = f"{base_url}/metadata/instance?api-version=2023-11-15"

        return check_platform(url, headers, azure)

    def get_hypervisor_vendor():
        command = "lscpu"
        output = execute_shell_command(command).decode("utf-8")
        lines = output.split("\n")
        for line in lines:
            if "Hypervisor vendor" in line:
                return line.split(":")[-1].strip().lower()
        return False

    if check_gcp() == gcp:
        return gcp
    elif check_aws() == aws or check_ce_vm_marker_file() == aws:
        return aws
    elif check_azure() == azure or check_ce_vm_marker_file() == azure:
        return azure
    else:
        hypervisor_vendor = get_hypervisor_vendor()
        if hypervisor_vendor in [vmware, microsoft]:
            return hypervisor_vendor
        return custom


def collect_host_details():
    try:
        print("\nChecking for host details, please wait for a few seconds...")
        global HOST_OS, COMPOSE_VERSION, CONTAINERIZATION_PLATFORM
        HOST_OS = get_os_name_and_major_version()
        AVAILABLE_INPUTS["CE_AS_VM"] = str(ce_as_vm_check())
        AVAILABLE_INPUTS["HA_ENABLED"] = "True" if is_ha else "False"
        AVAILABLE_INPUTS["CONTAINERIZATION_PLATFORM"] = (
            f'"{CONTAINERIZATION_PLATFORM}"'
        )
        AVAILABLE_INPUTS["COMPOSE_VERSION"] = f'"{COMPOSE_VERSION}"'
        AVAILABLE_INPUTS["HOST_OS"] = f'"{HOST_OS}"'
        AVAILABLE_INPUTS["PLATFORM_PROVIDER"] = str(
            collect_host_platform_details()
        )
    except Exception:
        pass


def check_and_apply_rabbitmq_flags():
    """
    Check and apply rabbitmq feature flags
    """
    try:
        print("\nChecking and applying rabbitmq feature flags, please wait for a few seconds...")
        CHECKS["RabbitMQ Feature Flags Check"] = Status.NOT_VERIFIED
        if not os.path.exists("data/rabbitmq/data/"):
            return
        for file in os.listdir("data/rabbitmq/data/"):
            if "feature_flags" in file:
                current_data = ""
                with open(f"data/rabbitmq/data/{file}", "r") as f:
                    current_data = f.read()
                if "stream_filtering" in current_data or "rabbitmq_4.0.0" in current_data:
                    continue
                with open(f"data/rabbitmq/data/{file}", "w") as f:
                    f.write(RABBITMQ_FEATURE_FLAGS)
        print_pass("Successfully applied rabbitmq feature flags")
        CHECKS["RabbitMQ Feature Flags Check"] = Status.PASS
    except Exception as e:
        print_warning(f"Error occurred while applying rabbitmq feature flags: {e}")
        print_warning(
            "You may need to delete the rabbitmq data from data/rabbitmq/data/ directory and before starting the Cloud Exchange."
        )


class CustomWFile:
    def __init__(self, content_length: int = None) -> None:
        self.content_length = content_length

    def write(self, b):
        if isinstance(b, bytes):
            b = b.decode("utf-8").strip()

        if self.content_length:
            splitted_line = b.split(":", maxsplit=1)  # log_level, log_message
            if len(splitted_line) > 1:
                print(splitted_line[1])
            self.content_length = None
        else:
            self.content_length = b

    def flush(self):
        pass

    def close(self):
        pass


class CustomHandler:
    def __init__(self):
        body = b"{}"
        self.headers = {"Content-Length": str(len(body))}
        self.rfile = io.BytesIO(body)
        self.wfile = CustomWFile()

    # If the original handler has more methods used elsewhere, define them here as pass or print
    def __getattr__(self, name):
        # This will catch any undefined method calls and mimic them
        def method(*args, **kwargs):
            pass

        return method

def verify_mount_status():
    result = subprocess.run(
        "df -h | grep CloudExchange | grep '/opt/shared/data'",
        shell=True,
        capture_output=True,
        text=True
    )
    return True if result.returncode == 0 else False


def setup_primary_volume(node_ip):
    # For upgrade scenario, check the mount status, if its mounted, skip the installation.
    configure_logger(
        log_file_max_bytes=CLOUD_EXCHANGE_CONFIG.get("LOG_FILE_MAX_BYTES", (10 * 1024 * 1024)),
        backup_count=CLOUD_EXCHANGE_CONFIG.get("LOG_FILE_BACKUP_COUNT", 5),
        log_file_name="setup.log",
        logs_directory="data/logs",
        should_add_stdout=False,
    )
    new_shared_directory_path = "/opt/shared"
    exists = verify_mount_status()
    if exists:
        print("Gluster volume for CloudExchange is already mounted at path '/opt/shared/data', skipping installation.")
        return
    custom_handler = CustomHandler()

    try:
        glusterfs_base_port = int(CLOUD_EXCHANGE_CONFIG.get("GLUSTERFS_BASE_PORT", GLUSTERFS_BASE_PORT))
        glusterfs_max_port = int(CLOUD_EXCHANGE_CONFIG.get("GLUSTERFS_MAX_PORT", GLUSTERFS_MAX_PORT))
    except Exception:
        glusterfs_base_port = GLUSTERFS_BASE_PORT
        glusterfs_max_port = GLUSTERFS_MAX_PORT

    response = install_gluster(
        handler=custom_handler,
        shared_directory_path=new_shared_directory_path,
        glusterfs_base_port=glusterfs_base_port,
        glusterfs_max_port=glusterfs_max_port,
        should_end_stream=False,
    )
    if response[1] != 200:
        print_fail("Failed to install glusterfs.")
        exit(1)
    # Verify gluster volume status
    response = verify_start_create_volume(
        handler=custom_handler, current_node_ip=node_ip
    )
    if response[1] != 200:
        print_fail("Failed to create gluster volume.")
        exit(1)
    # Verify volume mount
    response = ensure_volume_mounted(
        handler=custom_handler,
        shared_directory_path=new_shared_directory_path,
        current_node_ip=node_ip,
        should_end_stream=False,
    )
    if response[1] != 200:
        print_fail("Failed to mount gluster volume at `/opt/shared/data`.")
        exit(1)


def remove_passwords_from_config():
    update_data = {
        "MAINTENANCE_PASSWORD": "",
        "CE_SSL_CERTIFICATE_PASSWORD": "",
    }
    try:
        update_cloudexchange_config(updated_config=update_data)
        print(
            "Removed MAINTENANCE_PASSWORD and CE_SSL_CERTIFICATE_PASSWORD from cloudexchange.config file."
        )
    except Exception:
        print(
            "Error encountered while removing MAINTENANCE_PASSWORD and CE_SSL_CERTIFICATE_PASSWORD "
            "from cloudexchange.config file."
        )


def run():
    try:
        global should_ignore, avoid_service_restart, is_ha
        parser = argparse.ArgumentParser(
            prog="./setup",
            description="Setup the environment for the Netskope CE.",
            epilog="For more info, please read the user guide.",
        )
        parser.add_argument(
            "-l",
            "--location",
            help="Location of the NFS mount directory.",
            required=False,
            default="",
        )
        parser.add_argument(
            "--ignore-failures",
            nargs="?",
            const=True,
            default=False,
            help="Ignore failures and continue.",
        )
        parser.add_argument(
            "--avoid-service-restart",
            nargs="?",
            const=True,
            default=False,
            help="Avoid restarting the cloud-exchange service.",
        )
        parser.add_argument(
            "--check-prerequisites",
            nargs="?",
            choices=["SA", "HA"],
            default=None,
            help="Check prerequisites for deployment type (SA or HA). Defaults to None if not passed."
        )
        args = parser.parse_args()
        location = args.location.rstrip("/")
        should_ignore = args.ignore_failures
        avoid_service_restart = args.avoid_service_restart
        should_check_prerequisites = args.check_prerequisites

        print_banner()
        if not ce_as_vm_check():
            pull_from_github()
        if is_selinux_enabled():
            if not should_ignore:
                print_fail(
                    "SE Linux is enabled. Please DISABLE SE Linux for successful deployment of Netskope Cloud Exchange."
                )
                exit(1)
            else:
                print_warning(
                    "Ignoring the SE linux... '--ignore-failures' flag applied."
                )

        read_cloud_exchange_config_file()
        if should_check_prerequisites is not None:
            set_proxy()
            check_prerequisites()
            if isinstance(should_check_prerequisites, str) and should_check_prerequisites.strip() == "HA":
                check_python_prerequisites()
            if (Status.FAIL in CHECKS.values()) or (
                 Status.NOT_VERIFIED
                    in [
                        CHECKS.get("Podman-Plugin", None),
                        CHECKS.get("Podman", None),
                        CHECKS.get("Podman-Compose", None),
                    ]
                ):
                print(
                    "\n\n\033[1;31mVerification Failed. One or more prerequisite checks are not met. Please address the minimum requirements and re-run the setup script.\033[0;37m\n"
                )
                exit(1)
            exit(0)

        fetch_container_info()
        check_management_service_running()
        create_env_if_not_exist()
        set_sudo_prefix()

        if (location == "") and (
            CLOUD_EXCHANGE_CONFIG.get("HA_CURRENT_NODE") != CLOUD_EXCHANGE_CONFIG.get("HA_PRIMARY_NODE_IP")
        ):
            location = "/opt/shared/data"  # The mount path for the gluster is not configurable for user.

        if location:
            check_python_prerequisites()
            get_all_existed_env_variable()  # Get local env variables
            get_all_existed_env_variable(
                f"{location}/config/.env"
            )  # Get env variables from shared location
            get_all_existed_env_variable(f"{location}/config/.env.ceversion")

            AVAILABLE_INPUTS["HA_NFS_DATA_DIRECTORY"] = location
            ask_for_ha_values(primary=False)

            update_yaml_file("docker-compose")
            if not ce_as_vm_check():
                update_yaml_file("podman-compose")
        else:
            get_all_existed_env_variable()  # Get already existing env variables
            location = AVAILABLE_INPUTS.get("HA_NFS_DATA_DIRECTORY", "")
            current_node_ip = CLOUD_EXCHANGE_CONFIG.get("HA_PRIMARY_NODE_IP", "") or AVAILABLE_INPUTS.get("HA_PRIMARY_NODE_IP", "")
            if len(current_node_ip) > 0:
                setup_primary_volume(node_ip=current_node_ip)
                location = "/opt/shared/data"
                AVAILABLE_INPUTS["HA_NFS_DATA_DIRECTORY"] = location

            get_all_existed_env_variable(".env.ceversion")
            get_all_existed_env_variable(f"{location}/config/.env.ceversion")
            AVAILABLE_INPUTS.update(LATEST_TAG_AND_VERSION)
            if ce_as_vm_check():
                set_ce_version_for_vm()
            set_proxy()
            create_persistent_dirs()
            set_up_ha_values()
            if CLOUD_EXCHANGE_CONFIG.get("HA_ENABLED", False):
                location = AVAILABLE_INPUTS.get("HA_NFS_DATA_DIRECTORY")

        collect_host_details()
        check_prerequisites()
        migrate_mongo()
        check_and_apply_file_permissions()
        AVAILABLE_INPUTS["CONTAINERIZATION_PLATFORM"] = (
            f'"{CONTAINERIZATION_PLATFORM}"'
        )
        AVAILABLE_INPUTS["COMPOSE_VERSION"] = f'"{COMPOSE_VERSION}"'

        if (Status.FAIL in CHECKS.values()) or (
            Status.NOT_VERIFIED
            in [
                CHECKS.get("Podman-Plugin", None),
                CHECKS.get("Podman", None),
                CHECKS.get("Podman-Compose", None),
            ]
        ):
            print(
                "\n\n\033[1;31mInstallation failed. One or more prerequisite checks are not met. Please address the minimum requirements and re-run the setup script.\033[0;37m\n"
            )
            if should_ignore:
                print(
                    "\033[1;31mIgnoring the failures... '--ignore-failures' flag applied.\033[0;37m"
                )
            else:
                exit(1)

        if not args.location.rstrip("/"):
            set_config_params()

        assign_memory_to_rabbitmq_container()
        mpass = None
        if AVAILABLE_INPUTS.get("MAINTENANCE_PASSWORD", ""):
            mpass = AVAILABLE_INPUTS.get("MAINTENANCE_PASSWORD", "")
        elif AVAILABLE_INPUTS.get("LOCATION", "") and os.path.exists(
            AVAILABLE_INPUTS["LOCATION"]
        ):
            mpass = get_existing_variable_value(
                AVAILABLE_INPUTS["LOCATION"], "MAINTENANCE_PASSWORD"
            )
        if AVAILABLE_INPUTS.get("CE_SETUP_ID", None) and not AVAILABLE_INPUTS.get("MAINTENANCE_PASSWORD", None):
            mpass = change_maintenance_password(mpass, False)
        mongo_feature_compatibility(mpass)
        put_env_variable(AVAILABLE_INPUTS)
        check_and_apply_rabbitmq_flags()
        if location:
            AVAILABLE_INPUTS["ENV_FILE"] = f"{location}/config/.env"
            put_env_variable(AVAILABLE_INPUTS, AVAILABLE_INPUTS["ENV_FILE"])
            take_env_backup(AVAILABLE_INPUTS["ENV_FILE"], ".env.ha.backup")

        if location or is_ha:
            remove_unwanted_fields()
        put_env_variable(AVAILABLE_INPUTS)
        take_env_backup(".env", ".env.backup")
        secret_location = get_secret_location(AVAILABLE_INPUTS)
        if secret_location.strip():
            take_env_backup(secret_location.strip(), ".env.key.backup")
        check_and_apply_rabbitmq_flags()
        remove_passwords_from_config()  # Remove only on successfull setup.
        message = "\nSetup completed successfully..."
        print(f"{message}")

    except KeyboardInterrupt:
        if location:
            AVAILABLE_INPUTS["ENV_FILE"] = f"{location}/config/.env"
            put_env_variable(AVAILABLE_INPUTS, AVAILABLE_INPUTS["ENV_FILE"])
        if location or is_ha:
            remove_unwanted_fields()
        put_env_variable(AVAILABLE_INPUTS)
        print(
            "\nSetup stopped... The entered configurations will be saved...\n"
        )
    except Exception as e:
        print(e)
        exit(1)


def setup_systemd_service(service_name):
    """
    Creates, installs, enables, and starts a systemd service.
    :param service_name: Name of the systemd service (without .service extension)
    """
    working_directory = os.path.dirname(os.path.abspath(__file__))
    service_file_path = f"/etc/systemd/system/{service_name}.service"
    exec_command = os.path.join(working_directory, "start_management_server")
    service_content = f"""[Unit]
Description={service_name} Service
After=network.target

[Service]
User=root
WorkingDirectory={working_directory}
ExecStart={exec_command}
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
"""
    try:
        # Write the service file
        with open(service_file_path, "w") as f:
            f.write(service_content)

        # Set correct permissions
        os.chmod(service_file_path, 0o644)

        # Reload systemd, enable and start the service
        os.system("systemctl daemon-reload")
        os.system(f"systemctl enable {service_name}")
        os.system(f"systemctl restart {service_name}")

        print(f"Management service {service_name} has been installed and started successfully.")
    except Exception as e:
        print(f"Error setting up the Management service. Error: {e}")


def check_python_version():
    try:
        version_info = sys.version_info
        version_str = f"{version_info.major}.{version_info.minor}.{version_info.micro}"

        if version_info.major < MIN_PYTHON_MAJOR or (
            version_info.major == MIN_PYTHON_MAJOR
            and version_info.minor < MIN_PYTHON_MINOR
        ):
            print_fail(
                f"Requires Python {MIN_PYTHON_MAJOR}.{MIN_PYTHON_MINOR}+, found {version_str}"
            )
            sys.exit(1)

        print_pass(
            f"Python {version_str} meets requirement ({MIN_PYTHON_MAJOR}.{MIN_PYTHON_MINOR}+)"
        )

        try:
            pip_version = subprocess.run(
                [sys.executable, "-m", "pip", "--version"],
                capture_output=True,
                text=True,
                check=True,
            ).stdout.strip()
            print_pass(f"Pip: {pip_version}")
        except subprocess.CalledProcessError:
            print_fail("Could not determine pip version for Python3, Please install python3-pip or equivalent package for your system")
            sys.exit(1)

    except Exception as e:
        print_fail(f"Error checking Python version: {e}")
        sys.exit(1)


def check_venv_available():
    try:
        import venv
        print_pass("venv module is available")
    except ImportError:
        print_fail(
            "venv module is not available in Python3, Please install python3-venv or equivalent package for your system"
        )
        sys.exit(1)
    try:
        import ensurepip
        print_pass("ensurepip module is available")
    except ImportError:
        print_fail(
            "ensurepip module is not available in Python3, Please install python3-ensurepip or equivalent package for your system"
        )
        sys.exit(1)
    return True


def setup_virtual_environment():
    venv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), VENV_DIR)
    should_install_venv = True
    if os.path.exists(venv_path) and os.path.isdir(venv_path):
        print(f"Virtual environment '{VENV_DIR}' already exists.")
        print("Using existing virtual environment.")
        activate_script_path = os.path.join(venv_path, "bin", "activate")
        python_script_path = os.path.join(venv_path, "bin", "python")
        if not os.path.exists(activate_script_path) or not os.path.exists(python_script_path):
            print_fail(f"Could not found activate or python script in '{VENV_DIR}/bin'")
            should_install_venv = True
        else:
            should_install_venv = False

    if should_install_venv:
        print(f"Creating new virtual environment '{VENV_DIR}'...")
        try:
            subprocess.run([sys.executable, "-m", "venv", "--clear", VENV_DIR], check=True)
            print_pass("Virtual environment created successfully")
        except subprocess.CalledProcessError as e:
            print_fail(f"Failed to create virtual environment: {e}")
            sys.exit(1)


def ensure_environment():
    print("Checking Python version...")
    check_python_version()

    print("Checking if venv module is available...")
    check_venv_available()

    setup_virtual_environment()


if __name__ == "__main__":
    try:
        ensure_environment()
        run()
        if is_management_service_running and avoid_service_restart:
            print(
                f"Management service {CLOUD_EXCHANGE_LINUX_SERVICE_NAME} is running, skipping installation."
            )
        else:
            setup_systemd_service(
                CLOUD_EXCHANGE_LINUX_SERVICE_NAME
            )
    except Exception as e:
        print(e)
        exit(1)
